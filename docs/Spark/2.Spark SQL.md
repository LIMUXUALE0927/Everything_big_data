# Spark SQL

## DataFrame & DataSet

DataFrame 是一种**带 Schema 的分布式数据集**，在结构上类似于一张二维表。

DataFrame 背后的计算引擎是 Spark SQL，而 RDD 的计算引擎是 Spark Core。

DataFrame 与 RDD 的主要区别在于，DataFrame 带有 **schema 元信息**。

DataSet 是 DataFrame 的一个扩展。DataSet 是强类型的，支持泛型特性，可以让 Java、Scala 语言更好的利用。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211943343.png)

!!! question "有了 RDD 为什么还要 DataFrame？"

    在 RDD 开发框架下，Spark Core 的**优化空间受限**。绝大多数 RDD 高阶算子所封装的封装的计算逻辑（形参函数 f）对于 Spark Core 是透明的，Spark Core 除了用闭包的方式把函数 f 分发到 Executors 以外，没什么优化余地。而 DataFrame 的出现带来了新思路，它携带的 Schema 提供了丰富的类型信息，而且 DataFrame 算子大多为处理数据列的标量函数。DataFrame 的这两个特点，为引擎内核的优化打开了全新的空间。在 DataFrame 的开发框架下，负责具体优化过程的正是 Spark SQL。

!!! note "Spark Core 和 Spark SQL 的关系"

    Spark SQL，则是凌驾于 Spark Core 之上的一层优化引擎，它的主要职责，**是在用户代码交付 Spark Core 之前，对用户代码进行优化**。

    Spark Core 特指 Spark 底层执行引擎（Execution Engine），它包括了调度系统、存储系统、内存管理、Shuffle 管理等核心功能模块。而 Spark SQL 则凌驾于 Spark Core 之上，是一层独立的**优化引擎（Optimization Engine）**。换句话说，**Spark Core 负责执行，而 Spark SQL 负责优化，Spark SQL 优化过后的代码，依然要交付 Spark Core 来做执行**。

    再者，从开发入口来说，在 RDD 框架下开发的应用程序，会直接交付 Spark Core 运行。而使用 DataFrame API 开发的应用，则会先过一遍 Spark SQL，由 Spark SQL 优化过后再交由 Spark Core 去做执行。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211539272.png)

### RDD、DataFrame 和 DataSet 的异同

三者的共性：

- RDD、DataFrame、Dataset 都是 Spark 中的分布式弹性数据集，为处理海量数据提供便利
- 三者都有许多相同的概念，如分区、持久化、容错等；有许多共同的函数，如 map, filter, sortBy 等
- 三者都有惰性机制，只有在遇到 Action 算子时，才会开始真正的计算

三者的区别：

- 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，只有通过解析才能获取各个字段的值
- DataFrame 定义为 `Dataset[Row]`。每一行的类型是 Row，然后在 Row 包含具体的字段信息
- Dataset 每一行的类型都是一个 case class，在自定义了 case class 之后可以很自由的获得每一行的信息

---

## :fire: Catalyst 优化器

基于 DataFrame，Spark SQL 是如何进行优化的？

- Catalyst 优化器
- Tungsten

Catalyst 优化器，它的职责在于创建并优化执行计划，它包含 3 个功能模块，分别是**创建语法树并生成执行计划**、**逻辑阶段优化**和**物理阶段优化**。

- 创建 AST：Parser
- 生成逻辑执行计划：Analyzer
- 逻辑阶段优化：RBO（基于规则优化）
- 物理阶段优化：CBO（基于代价优化）

Tungsten 用于衔接 Catalyst 执行计划与底层的 Spark Core 执行引擎，它主要负责优化数据结构与可执行代码。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211542778.png)

**逻辑阶段优化：**

像**谓词下推**、**列剪枝**这样的特性，都被称为启发式的规则或策略。而 Catalyst 优化器的核心职责之一，就是在逻辑优化阶段，基于启发式的规则和策略调整、优化执行计划，为物理优化阶段提升性能奠定基础。

**物理阶段优化：**

Catalyst 在物理优化阶段还会进一步优化执行计划。与逻辑阶段主要依赖先验的启发式经验不同，物理阶段的优化，主要依赖各式各样的**统计信息**，如数据表尺寸、是否启用数据缓存、Shuffle 中间文件，等等。换句话说，逻辑优化更多的是一种“经验主义”，而物理优化则是“用数据说话”。

以 Join 为例，执行计划仅交代了两张数据表需要做关联，但是，它并没有交代清楚这两张表具体采用哪种机制来做关联。按照实现机制来分类，数据关联有 3 种实现方式，分别是**嵌套循环连接（NLJ，Nested Loop Join）**、**排序归并连接（Sort Merge Join）**和**哈希连接（Hash Join）**。而按照数据分发方式来分类，数据关联又可以分为 **Shuffle Join** 和 **Broadcast Join** 这两大类。因此，在分布式计算环境中，至少有 6 种 Join 策略供 Spark SQL 来选择。

---

Catalyst 优化器工作流程图

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311230110271.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311260010008.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311260011490.png)

**Parser 模块**：将 SparkSQL 字符串解析为一个抽象语法树/AST 。

- Parser 模块目前都是使用第三方类库**ANTLR**进行实现的
- 在这个过程中，会判断 SQL 语句是否符合规范，比如 select from where 等这些关键字是否写对。

**Analyzer 模块**：该模块会遍历整个 AST，并对 AST 上的每个节点进行**数据类型绑定以及函数绑定**，然后根据元数据信息 Catalog 对数据表中的字段进行解析。

此过程就会判断 SQL 语句的表名，字段名是否真的在元数据库里存在。

元数据信息主要包括两部分：**表的 Scheme**和**基本函数信息**

- 表的 Scheme：包括表的基本定义（列名、数据类型）、表的数据格式（Json、Text）、表的物理位置等
- 基本函数：主要指类信息

**Optimizer 模块**：主要分为 RBO 和 CBO 两种优化策略，其中**RBO 是基于规则优化，CBO 是基于代价优化**。常见的规则有：

- **谓词下推 Predicate Pushdown：将过滤操作下推到 join 之前进行**，之后再进行 join 的时候，数据量将会得到显著的减少，join 耗时必然降低。
- **列值裁剪 Column Pruning：**在谓词下推后,可以**把表中没有用到的列裁剪掉，**这一优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列式存储数据库来说大大提高了扫描效率
- **常量累加 Constant Folding：**比如计算 `x+(100+80) -> x+180`，虽然是一个很小的改动，但是意义巨大。如果没有进行优化的话，每一条结果都需要执行一次`100+80`的操作，然后再与结果相加。优化后就不需要再次执行 `100+80` 操作。

**SparkPlanner 模块**：将优化后的逻辑执行计划（OptimizedLogicalPlan）转换成**physical plan（物理计划），**也就是 Spark 可以真正执行的计划。

比如 join 算子，Spark 根据不同场景为该算子制定了不同的算法策略，有 `BroadcastHashJoin`、`ShuffleHashJoin` 以及 `SortMergejoin` 等，物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的算法实现，怎么挑选，下面简单说下：

- SparkPlanner 对优化后的逻辑计划进行转换，是生成了**多个可以执行的物理计划 Physical Plan**；
- 接着**CBO（基于代价优化）**优化策略会根据**Cost Model**算出每个 Physical Plan 的代价，并选取代价最小的 Physical Plan 作为最终的 Physical Plan。
- **CostModel 模块**：主要根据过去的性能统计数据，选择最佳的物理执行计划。这个过程的优化就是 CBO（基于代价优化）

**执行物理计划：**最后依据最优的物理执行计划，生成 java 字节码，将 SQL 转化为 DAG，以 RDD 形式进行操作。

---

Catalyst 的一大好处是：相比于 RDD，使用 DataFrame 可以使得不同编程语言的执行效率接近，因为 DataFrame 的执行计划是一样的。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311260013592.png)

---

## Tungsten

Tungsten 主要是在**数据结构**和**执行代码**这两个方面，做进一步的优化。数据结构优化指的是 **Unsafe Row** 的设计与实现，执行代码优化则指的是**全阶段代码生成**（WSCG，Whole Stage Code Generation）。

我们先来看看为什么要有 Unsafe Row。对于 DataFrame 中的每一条数据记录，Spark SQL 默认采用 org.apache.spark.sql.Row 对象来进行封装和存储。我们知道，使用 Java Object 来存储数据会引入大量额外的存储开销。

为此，Tungsten 设计并实现了一种叫做 Unsafe Row 的二进制数据结构。**Unsafe Row 本质上是字节数组，它以极其紧凑的格式来存储 DataFrame 的每一条数据记录，大幅削减存储开销，从而提升数据的存储与访问效率。**

接下来，我们再来说说 WSCG：全阶段代码生成。所谓全阶段，其实就是我们在调度系统中学过的 Stage。以图中的执行计划为例，标记为绿色的 3 个节点，在任务调度的时候，会被划分到同一个 Stage。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211551076.png)

而代码生成，指的是 Tungsten **在运行时把算子之间的「链式调用」捏合为一份代码**。经过了 Tungsten 的 WSCG 优化之后，Filter、Select 和 Scan 这 3 个算子，会被「捏合」为一个函数 f。这样一来，Spark Core 只需要使用函数 f 来一次性地处理每一条数据，就能**减少不同算子之间数据通信的开销**，一气呵成地完成计算。

---

## Spark Session

The entry point to programming Spark with the Dataset and DataFrame API.

Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 **SparkContext**， Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。**SparkSession** 是 Spark 最新的 SQL 查询起始点。

```scala
SparkSession.builder
  .master("local")
  .appName("Word Count")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()
```

```scala
import org.apache.spark.sql.functions._

val spark = SparkSession.builder()
  .master("local")
  .appName("test")
  .getOrCreate()
val sc = spark.sparkContext
import spark.implicits._
```

---

## 创建 DataFrame

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211601485.png)

### 从 Driver 创建 DataFrame

**方法一：createDataFrame 方法**

```scala
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))

import org.apache.spark.sql.functions._

val df = spark.createDataFrame(seq)
  .withColumnRenamed("_1", "name")
  .withColumnRenamed("_2", "age")
  .orderBy(desc("age")) // desc 来自于 org.apache.spark.sql.functions._

df.show()
df.printSchema()
```

**方法二：toDF 方法**

我们显示导入了 `spark.implicits` 包中的所有方法，然后通过在集合或 RDD 之上调用 toDF 就能轻松创建 DataFrame。

```scala
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))
val df = spark.createDataFrame(seq).toDF("name", "age")

df.show()
df.printSchema()
```

### 从文件系统创建 DataFrame

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211620376.png)

**以 CSV 为例：**

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211623003.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211623290.png)

**以 Parquet / ORC 为例：**

Parquet 与 ORC，都是应用广泛的**列存（Column-based Store）文件格式**。在传统的行存文件格式中，数据记录以行为单位进行存储。虽然这非常符合人类的直觉，**但在数据的检索与扫描方面，行存数据往往效率低下**。例如，在数据探索、数据分析等数仓应用场景中，我们往往仅需扫描数据记录的某些字段，但在行存模式下，我们必须要扫描全量数据，才能完成字段的过滤。

很多列存格式往往**在文件中记录 Data Schema**，比如 Parquet 和 ORC，它们会利用 Meta Data 数据结构，来记录所存储数据的数据模式。这样一来，在读取类似列存文件时，我们无需再像读取 CSV 一样，去手工指定 Data Schema。

### 从 RDBMS 创建 DataFrame

```scala
spark.read.format("jdbc")
  .option("driver", "com.mysql.jdbc.Driver")
  .option("url", "jdbc:mysql://hostname:port/mysql")
  .option("user", "用户名")
  .option("password","密码")
  .option("numPartitions", 20)
  .option("dbtable", "数据表名 ")
  .load()
```

```scala
val sqlQuery: String = “select * from users where gender = ‘female’”
spark.read.format("jdbc")
  .option("driver", "com.mysql.jdbc.Driver")
  .option("url", "jdbc:mysql://hostname:port/mysql")
  .option("user", "用户名")
  .option("password","密码")
  .option("numPartitions", 20)
  .option("dbtable", sqlQuery)
  .load()
```

---

## 创建 DataSet

```scala
def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {...}
```

This method requires an **encoder** (**to convert a JVM object of type T to and from the internal Spark SQL representation**).

```scala
import spark.implicits._

case class Person(name: String, age: Int)

val seq = List(Person("Alice", 18), Person("Bob", 20), Person("Tom", 30))

val ds = spark.createDataset(seq)

ds.show()
ds.printSchema()
```

使用 case class 的好处是，**Spark SQL 会自动推断出 case class 的字段名和字段类型**。

---

## RDD、DataFrame、DataSet 的转换

### RDD to DataFrame

方法一：

```scala
import spark.implicits._
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))
val rdd = sc.makeRDD(seq)
val df = rdd.toDF("name", "age")

df.printSchema()
df.show()

val rdd2: RDD[Row] = df.rdd
```

方法二：

```scala
import spark.implicits._
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))

val rdd = sc.makeRDD(seq)
  .map(x => Row(x._1, x._2))

val schema = new StructType()
  .add("name", StringType, nullable = true)
  .add("age", IntegerType, nullable = true)

val df = spark.createDataFrame(rdd, schema)

df.printSchema()
df.show()

val rdd2: RDD[Row] = df.rdd
```

---

### RDD to DataSet

```scala
import spark.implicits._
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))

case class Person(name: String, age: Int)

val rdd = sc.makeRDD(seq)
val ds = rdd.map(x => Person(x._1, x._2)).toDS()

ds.printSchema()
ds.show()

val rdd2: RDD[Row] = ds.rdd
```

---

### DataFrame to DataSet

```scala
import spark.implicits._
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))
val df = seq.toDF("name", "age")
val ds: Dataset[Row] = df.as("alias here")
```

---

### DataSet to DataFrame

```scala
import spark.implicits._
val seq = List(("Alice", 18), ("Bob", 20), ("Tom", 30))

case class Person(name: String, age: Int)

val rdd = sc.makeRDD(seq)
val ds = rdd.map(x => Person(x._1, x._2)).toDS()
val df = ds.toDF()
```

---

## DataFrame 算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211628692.png)

### 同源类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211629921.png)

### 探索类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211630809.png)

### 清洗类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211630806.png)

表格中的最后一个算子是 na，它的作用是选取 DataFrame 中的 null 数据，na 往往要结合 drop 或是 fill 来使用。例如，`employeesDF.na.drop` 用于删除 DataFrame 中带 null 值的数据记录，而 `employeesDF.na.fill(0)` 则将 DataFrame 中所有的 null 值都自动填充为整数零。

### 转换类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211632149.png)

select 算子在功能方面不够灵活。在灵活性这方面，selectExpr 做得更好。比如说，基于 id 和姓名，我们想把它们拼接起来生成一列新的数据。像这种需求，正是 selectExpr 算子的用武之地。

```scala
employeesDF.selectExpr("id", "name", "concat(id, '_', name) as id_name").show

/** 结果打印
+---+-------+---------+
| id| name| id_name|
+---+-------+---------+
| 1| John| 1_John|
| 2| Lily| 2_Lily|
| 3|Raymond|3_Raymond|
+---+-------+---------+
*/
```

withColumnRenamed 是重命名现有的数据列，而 withColumn 则用于生成新的数据列，这一点上，withColumn 倒是和 selectExpr 有着异曲同工之妙。withColumn 也可以充分利用 Spark SQL 提供的 Built-in Functions 来灵活地生成数据。比如，基于年龄列，我们想生成一列脱敏数据，隐去真实年龄，你就可以这样操作。

```scala
employeesDF.withColumn("crypto", hash($"age")).show

/** 结果打印
+---+-------+---+------+-----------+
| id| name|age|gender| crypto|
+---+-------+---+------+-----------+
| 1| John| 26| Male|-1223696181|
| 2| Lily| 28|Female|-1721654386|
| 3|Raymond| 30| Male| 1796998381|
+---+-------+---+------+-----------+
*/
```

```scala
val seq = Seq( (1, "John", 26, "Male", Seq("Sports", "News")),
  (2, "Lily", 28, "Female", Seq("Shopping", "Reading")),
  (3, "Raymond", 30, "Male", Seq("Sports", "Reading"))
)

val employeesDF: DataFrame = seq.toDF("id", "name", "age", "gender", "interests")
employeesDF.show

/** 结果打印
+---+-------+---+------+-------------------+
| id| name|age|gender| interests|
+---+-------+---+------+-------------------+
| 1| John| 26| Male| [Sports, News]|
| 2| Lily| 28|Female|[Shopping, Reading]|
| 3|Raymond| 30| Male| [Sports, Reading]|
+---+-------+---+------+-------------------+
*/

employeesDF.withColumn("interest", explode($"interests")).show

/** 结果打印
+---+-------+---+------+-------------------+--------+
| id| name|age|gender| interests|interest|
+---+-------+---+------+-------------------+--------+
| 1| John| 26| Male| [Sports, News]| Sports|
| 1| John| 26| Male| [Sports, News]| News|
| 2| Lily| 28|Female|[Shopping, Reading]|Shopping|
| 2| Lily| 28|Female|[Shopping, Reading]| Reading|
| 3|Raymond| 30| Male| [Sports, Reading]| Sports|
| 3|Raymond| 30| Male| [Sports, Reading]| Reading|
+---+-------+---+------+-------------------+--------+
*/
```

### 分析类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211635230.png)

### 持久化类算子

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211637013.png)

在 write API 中，mode 用于指定“写入模式”，分别有 Append、Overwrite、ErrorIfExists、Ignore 这 4 种模式，它们的含义与描述如下表所示。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211637410.png)

---

## Join 的实现机制和策略

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211843173.png)

Join 有 3 种实现机制，分别是 **NLJ（Nested Loop Join）**、**SMJ（Sort Merge Join）**和 **HJ（Hash Join）**。

在分布式环境下，从数据分发模式来看，Join 又可以分为 **Shuffle Join** 和 **Broadcast Join** 这两大类。因此，**在分布式计算环境中，至少有 6 种 Join 策略供 Spark SQL 来选择**。

---

### Nested Loop Join

对于参与关联的两张表，如 salaries 和 employees，按照它们在代码中出现的顺序，我们约定俗成地把 salaries 称作“左表”，而把 employees 称作“右表”。在探讨关联机制的时候，我们又常常把左表称作是“驱动表”，而把右表称为“基表”。

一般来说，驱动表的体量往往较大，在实现关联的过程中，驱动表是主动扫描数据的那一方。而基表相对来说体量较小，它是被动参与数据扫描的那一方。

在 NLJ 的实现机制下，算法会使用外、内两个嵌套的 for 循环，来依次扫描驱动表与基表中的数据记录。在扫描的同时，还会判定关联条件是否成立，如内关联例子中的 `salaries("id") === employees("id")`。如果关联条件成立，就把两张表的记录拼接在一起，然后对外进行输出。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211833130.png)

不难发现，假设驱动表有 M 行数据，而基表有 N 行数据，那么 NLJ 算法的计算复杂度是 $O(M * N)$。尽管 NLJ 的实现方式简单、直观、易懂，但它的执行效率显然很差。

---

### Sort Merge Join

鉴于 NLJ 低效的计算效率，SMJ 应运而生。Sort Merge Join，顾名思义，SMJ 的实现思路是**先排序、再归并**。给定参与关联的两张表，SMJ 先把他们各自排序，然后再使用独立的游标，对排好序的两张表做归并关联。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211835410.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211838341.png)

SMJ 算法的计算复杂度为 $O(MlogM + NlogN)$。如果两张表已排序，那么 SMJ 的计算复杂度就是 $O(M + N)$。

---

### Hash Join

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211841081.png)

具体来说，HJ 的计算分为两个阶段，分别是 Build 阶段和 Probe 阶段。在 Build 阶段，在基表之上，算法使用既定的哈希函数构建哈希表，如上图的步骤 1 所示。哈希表中的 Key 是 id 字段应用（Apply）哈希函数之后的哈希值，而哈希表的 Value 同时包含了原始的 Join Key（id 字段）和 Payload。

在 Probe 阶段，算法依次遍历驱动表的每一条数据记录。首先使用同样的哈希函数，以动态的方式计算 Join Key 的哈希值。然后，算法再用哈希值去查询刚刚在 Build 阶段创建好的哈希表。如果查询失败，则说明该条记录与基表中的数据不存在关联关系；相反，如果查询成功，则继续对比两边的 Join Key。如果 Join Key 一致，就把两边的记录进行拼接并输出，从而完成数据关联。

---

### 总结

Join 支持 3 种实现机制，它们分别是 Hash Join、Sort Merge Join 和 Nested Loop Join。三者之中，Hash Join 的执行效率最高，这主要得益于哈希表 $O(1)$ 的查找效率。不过，在 Probe 阶段享受哈希表的“性能红利”之前，Build 阶段得先在内存中构建出哈希表才行。因此，**Hash Join 这种算法对于内存的要求比较高，适用于内存能够容纳基表数据的计算场景。**

相比之下，Sort Merge Join 就没有内存方面的限制。不论是排序、还是合并，SMJ 都可以利用磁盘来完成计算。所以，在稳定性这方面，SMJ 更胜一筹。

与前两者相比，Nested Loop Join 看上去有些多余，嵌套的双层 for 循环带来的计算复杂度最高：$O(M * N)$。不过，尺有所短寸有所长，**执行高效的 HJ 和 SMJ 只能用于等值关联，也就是说关联条件必须是等式**，像 `salaries("id") < employees("id")` 这样的关联条件，HJ 和 SMJ 是无能为力的。相反，**NLJ 既可以处理等值关联（Equi Join），也可以应付不等值关联（Non Equi Join）**，可以说是数据关联在实现机制上的最后一道防线。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211901706.png)

---

### Shuffle Join

Spark SQL 默认采用 Shuffle Join 来完成分布式环境下的数据关联。对于参与 Join 的两张数据表，Spark SQL 先是按照如下规则，来决定不同数据记录应当分发到哪个 Executors 中去：

- 根据 Join Keys 计算哈希值
- 将哈希值对并行度（Parallelism）取模

由于左表与右表在并行度（分区数）上是一致的，因此，按照同样的规则分发数据之后，一定能够保证 id 字段值相同的薪资数据与员工数据坐落在同样的 Executors 中。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311220441835.png)

经过 Shuffle 过后，Join Keys 相同的记录被分发到了同样的 Executors 中去。接下来，在 Reduce 阶段，Reduce Task 就可以使用 HJ、SMJ、或是 NLJ 算法在 Executors 内部完成数据关联的计算。

---

### Broadcast Join

实际上，Spark 不仅可以在普通变量上创建广播变量，**在分布式数据集（如 RDD、DataFrame）之上也可以创建广播变量**。这样一来，对于参与 Join 的两张表，我们可以把其中较小的一个封装为广播变量，然后再让它们进行关联。

```scala
import org.apache.spark.sql.functions.broadcast

// 创建员工表的广播变量
val bcEmployees = broadcast(employees)

// 内关联，PS：将原来的employees替换为bcEmployees
val jointDF: DataFrame = salaries.join(bcEmployees, salaries("id") === employees("id"), "inner")
```

在 Broadcast Join 的执行过程中，Spark SQL 首先从各个 Executors 收集 employees 表所有的数据分片，然后在 Driver 端构建广播变量 bcEmployees。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311220441854.png)

尽管广播变量的创建与分发同样需要消耗网络带宽，但相比 Shuffle Join 中两张表的全网分发，因为仅仅**通过分发体量较小的数据表来完成数据关联**，Spark SQL 的执行性能显然要高效得多。

---

### Join 策略选择

不论是 Shuffle Join，还是 Broadcast Join，一旦数据分发完毕，理论上可以采用 HJ、SMJ 和 NLJ 这 3 种实现机制中的任意一种，完成 Executors 内部的数据关联。因此，两种分发模式，与三种实现机制，它们组合起来，总共有 6 种分布式 Join 策略，如下图所示。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211857818.png)

注意：上图中的 Broadcast SMJ 是灰色的。这是因为 Spark 在能够 Broadcast 其中一张表的前提条件成立的情况下，会优先选择 Broadcast HJ 来完成数据关联，而不会选择性能更差的 Broadcast SMJ。因此 Spark 中实际可用的 Join 策略只有 5 种：

- Broadcast HJ：最优先选择
- Shuffle HJ
- Shuffle SMJ
- Broadcast NLJ
- Shuffle NLJ：又称为 Cartesian Product Join

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211858363.png)

```mermaid
flowchart TD
    A["Join"] --> B{"小表能否存入内存？"}
    B -->|"Yes"| C["Broadcast Join"]
    C --> I{"等值连接？"}
    I -->|"Yes"| J["Broadcast HJ"]
    I -->|"No"| K["Broadcast NLJ"]
    B ---->|"No"| E["Shuffle Join"]
    E --> F{"等值连接？"}
    F -->|"Yes"| G["Shuffle SMJ/Shuffle HJ"]
    F -->|"No"| H["Shuffle NLJ"]
```

**不论是等值关联、还是不等值关联，只要 Broadcast Join 的前提条件成立，Spark SQL 一定会优先选择 Broadcast Join 相关的策略。**

**Broadcast Join 得以实施的基础，是被广播数据表的全量数据能够完全放入 Driver 的内存、以及各个 Executors 的内存。**

另外，为了避免因广播表尺寸过大而引入新的性能隐患，Spark SQL 要求被广播表的内存大小不能超过 8GB。

当然，在 Broadcast Join 前提条件不成立的情况下，Spark SQL 就会退化到 Shuffle Join 的策略。在不等值的数据关联中，Spark SQL 只有 Shuffle NLJ 这一种选择。

学习过 Shuffle 之后，我们知道，Shuffle 在 Map 阶段往往会对数据做排序，而这恰恰正中 SMJ 机制的下怀。对于已经排好序的两张表，SMJ 的复杂度是 $O(M + N)$，这样的执行效率与 HJ 的 $O(M)$ 可以说是不相上下。再者，SMJ 在执行稳定性方面，远胜于 HJ，在内存受限的情况下，SMJ 可以充分利用磁盘来顺利地完成关联计算。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202311211900112.png)

!!! question "Broadcast Join 一定比 Shuffle Join 快吗？"

    在进行 Broadcast Join 之前，Spark 需要把处于 Executor 端的数据先发送到 Driver 端，然后 Driver 端制作哈希表之后再把数据广播到 Executor 端。如果我们需要广播的表的数据比较多，那么这个过程就会比较耗时，而且还会占用大量的网络带宽，甚至也有 OOM 的风险。因此，**Broadcast Join 并不一定比 Shuffle Join 快**。

---

### 源码

在 `SparkStrategies.scala` 中，我们可以看到 Spark SQL 中的 Join 策略是如何实现的：

Select the proper physical plan for join based on join strategy hints, the availability of equi-join keys and the sizes of joining relations. Below are the existing join strategies, their characteristics and their limitations.

- **Broadcast hash join (BHJ)**: ==Only supported for equi-joins==, while the join keys do not need to be sortable. ==Supported for all join types except full outer joins==. BHJ usually performs faster than the other join algorithms when the broadcast side is small. However, broadcasting tables is a network-intensive operation and it could cause OOM or perform badly in some cases, especially when the build/broadcast side is big.
- **Shuffle hash join**: ==Only supported for equi-joins==, while the join keys do not need to be sortable. Supported for all join types. Building hash map from table is a memory-intensive operation and it could cause OOM when the build side is big.
- **Shuffle sort merge join (SMJ)**: ==Only supported for equi-joins and the join keys have to be sortable==. Supported for all join types.
- **Broadcast nested loop join (BNLJ)**: Supports both equi-joins and non-equi-joins. Supports all the join types, but the implementation is optimized for: 1) broadcasting the left side in a right outer join; 2) broadcasting the right side in a left outer, left semi, left anti or existence join; 3) broadcasting either side in an inner-like join. For other cases, we need to scan the data multiple times, which can be rather slow.
- **Shuffle-and-replicate nested loop join** (a.k.a. cartesian product join): Supports both equi-joins and non-equi-joins. ==Supports only inner like joins==.

---

**If it is an equi-join**, we first look at the join hints w.r.t. the following order:

1.  broadcast hint: pick broadcast hash join if the join type is supported. If both sides
    have the broadcast hints, choose the smaller side (based on stats) to broadcast.
2.  sort merge hint: pick sort merge join if join keys are sortable.
3.  shuffle hash hint: We pick shuffle hash join if the join type is supported. If both
    sides have the shuffle hash hints, choose the smaller side (based on stats) as the
    build side.
4.  shuffle replicate NL hint: pick cartesian product if join type is inner like.

**If there is no hint or the hints are not applicable**, we follow these rules one by one:

1.  Pick broadcast hash join if one side is small enough to broadcast, and the join type
    is supported. If both sides are small, choose the smaller side (based on stats)
    to broadcast.
2.  Pick shuffle hash join if one side is small enough to build local hash map, and is
    much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false.
3.  Pick sort merge join if the join keys are sortable.
4.  Pick cartesian product if join type is inner like.
5.  Pick broadcast nested loop join as the final solution. It may OOM but we don't have
    other choice.

---

## Spark SQL 的表类型

在 Spark SQL 中，表的类型分为 3 种，分别是：

- 管理表（Managed Table）
- 外部表（External Table）
- 视图（View）

```scala
case class CatalogTableType private(name: String)
object CatalogTableType {
  val EXTERNAL = new CatalogTableType("EXTERNAL")
  val MANAGED = new CatalogTableType("MANAGED")
  val VIEW = new CatalogTableType("VIEW")

  val tableTypes = Seq(EXTERNAL, MANAGED, VIEW)
}
```

和 Hive 类似，管理表是由 Spark SQL 管理其元数据和数据存储的表。当你创建一个管理表时，Spark 会将表的元数据（如表结构、列类型等）和数据存储在默认的存储位置中，通常是在 Hive 的仓库中（默认是在 HDFS 上）。当你删除一个管理表时，Spark 会自动删除表的元数据和数据。

外部表的元数据由 Spark 管理，但数据存储在外部存储系统中，例如 HDFS、S3、HBase 等。创建无管理表时，你需要指定数据的存储位置（path），Spark 会将元数据保存在其内部，但不会管理数据的生命周期。这意味着当你删除一个无管理表时，Spark 只会删除元数据，而不会删除实际的数据。

视图既可以是全局的（对给定集群的所有 SparkSession 可见），也可以是会话级别的（只对单个 SparkSession 可见）。会话级别的视图就是临时视图，这些视图随 Spark 应用结束而消失。视图和表的区别是，视图不会存放实际数据。

---

## 缓存

用户可以像缓存 RDD 那样缓存 Spark SQL 中表与视图，或删除缓存。在 Spark 3.0 中，除了缓存命令原有的一些选项，还可以将表声明为 LAZY，这样它就不会立即被缓存，直到第一次遇到 Action 算子时才会真正被缓存。

```scala
val df = spark.read.option("header", value = true).csv("src/main/resources/data/people.csv")
df.cache()
df.createTempView("people")
spark.sqlContext.cacheTable("people")
spark.sqlContext.uncacheTable("people")
// SQL 方式
spark.sql("CACHE TABLE people")
spark.sql("UNCACHE TABLE people")
spark.sql("CACHE LAZY TABLE people")
```

---

## 元数据管理

**Catalog 是 Spark SQL 中的元数据管理组件**，它负责管理表、视图、函数等元数据信息。Catalog 有两种实现，分别是 HiveMetastoreCatalog 和 SessionCatalog。前者是对 Hive Metastore 的封装，后者是对内置元数据管理的封装。

Catalog 是 Spark 2.0 之后提供的访问元数据的类，其提供一些 API 用来对数据库、表、视图、缓存、列、函数（UDF/UDAF）进行操作。

```scala
val spark = SparkSession.builder()
  .master("local")
  .appName("catalog")
  .getOrCreate()
val catalog = spark.catalog
```

查看数据库信息：

```scala
val catalog = spark.catalog
spark.sql("show databases").show()
spark.catalog.listDatabases().show()
```

---

## Spark on Hive & Hive on Spark

### Spark on Hive

在 Spark 中使用 Hive 的功能。Spark 提供了一个称为 HiveContext（在 Spark 2.0+ 版本中被 SparkSession 取代）的 API，通过该 API 可以使用 Hive 的元数据和查询语言（HiveQL）来访问和操作 Hive 表。

Spark on Hive 能够在 Spark 中使用 Hive 的表和数据，以及执行 HiveQL 查询。它允许你使用 Spark 的强大分布式计算功能，并与 Hive 集成。

### Hive on Spark

指在 Hive 中使用 Spark 的执行引擎。在传统的 Hive 中，查询通常使用 MapReduce 作为执行引擎，但这可能导致较高的延迟。为了改善查询性能，Hive 也提供了一个称为 SparkExecutionEngine 的选项，该选项将查询的执行转移到 Spark 引擎上。可以通过使用 `set hive.execution.engine=spark` 的方式切换 Hive 底层的执行引擎。使用 Hive on Spark，你可以利用 Spark 的内存计算和并行处理能力来加速 Hive 查询。这种方式允许你在 Hive 中继续使用 HiveQL，但底层的执行引擎变为 Spark。

---

## UDF & UDAF

用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能。

```scala
object UDF {
  def main(args: Array[String]): Unit = {
    // 创建运行环境
    val spark = SparkSession.builder()
      .master("local")
      .appName("test")
      .getOrCreate()
    val sc = spark.sparkContext
    import spark.implicits._

    val df = spark.read.json("data/user.json")
    df.createOrReplaceTempView("user")

    spark.udf.register("prefixName", (name: String) => {
      "Name: " + name
    })

    spark.sql("select age, prefixName(username) from user").show

    //关闭环境
    spark.close()
  }
}
```

强类型的 Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从 Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数 Aggregator。

---

## AQE

在 2.0 版本之前，Spark SQL 仅仅支持启发式、静态的优化过程。

启发式的优化又叫 RBO（Rule Based Optimization，基于规则的优化），它往往基于一些规则和策略实现，如谓词下推、列剪枝，这些规则和策略来源于数据库领域已有的应用经验。也就是说，启发式的优化实际上算是一种经验主义。

CBO 的特点是“实事求是”，基于数据表的统计信息（如表大小、数据列分布）来选择优化策略。CBO 支持的统计信息很丰富，比如数据表的行数、每列的基数（Cardinality）、空值数、最大值、最小值和直方图等等。因为有统计数据做支持，所以 CBO 选择的优化策略往往优于 RBO 选择的优化规则。

但是，CBO 也面临三个方面的窘境：“窄、慢、静”。窄指的是适用面太窄，CBO 仅支持注册到 Hive Metastore 的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如 Parquet、ORC、CSV 等等。

慢指的是统计信息的搜集效率比较低。对于注册到 Hive Metastore 的数据表，开发者需要调用 ANALYZE TABLE COMPUTE STATISTICS 语句收集统计信息，而各类信息的收集会消耗大量时间。

静指的是静态优化，这一点与 RBO 一样。CBO 结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO 的使命就算完成了。换句话说，如果在运行时数据分布发生动态变化，CBO 先前制定的执行计划并不会跟着调整、适配。

**而 AQE 可以让 Spark 在运行时的不同阶段，结合实时的运行时状态，周期性地动态调整前面的逻辑计划，然后根据再优化的逻辑计划，重新选定最优的物理计划，从而调整运行时后续阶段的执行方式**。

---

**AQE 是 Spark SQL 的一种动态优化机制，在运行时，每当 Shuffle Map 阶段执行完毕，AQE 都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。**

**AQE 优化机制触发的时机是 Shuffle Map 阶段执行完毕。也就是说，AQE 优化的频次与执行计划中 Shuffle 的次数一致**。反过来说，如果你的查询语句不会引入 Shuffle 操作，那么 Spark SQL 是不会触发 AQE 的。

首先，AQE 赖以优化的统计信息与 CBO 不同，这些统计信息并不是关于某张表或是哪个列，而是 **Shuffle Map 阶段输出的中间文件**。我们知道，每个 Map Task 都会输出以 data 为后缀的数据文件，还有以 index 为结尾的索引文件，这些文件统称为中间文件。每个 data 文件的大小、空文件数量与占比、每个 Reduce Task 对应的分区大小，所有这些基于中间文件的统计值构成了 AQE 进行优化的信息来源。

其次，AQE 从运行时获取统计信息，在条件允许的情况下，**优化决策会分别作用到逻辑计划和物理计划**。

**AQE 既定的规则和策略主要有 4 个，分为 1 个逻辑优化规则和 3 个物理优化策略**。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202312140320449.png)

- Join 策略调整：如果某张表在过滤之后，尺寸小于广播变量阈值，这张表参与的数据关联就会从 Shuffle Sort Merge Join 降级（Demote）为执行效率更高的 Broadcast Hash Join。
- 自动分区合并：在 Shuffle 过后，Reduce Task 数据分布参差不齐，AQE 将自动合并过小的数据分区。
- 自动倾斜处理：结合配置项，AQE 自动拆分 Reduce 阶段过大的数据分区，降低单个 Reduce Task 的工作负载。

---

## DPP

DPP（Dynamic Partition Pruning，动态分区剪裁）是 Spark 3.0 版本中第二个引人注目的特性，它指的是在星型数仓的数据关联场景中，可以充分利用过滤之后的维度表，大幅削减事实表的数据扫描量，从整体上提升关联计算的执行性能。

我们先来看这个例子。在星型（Start Schema）数仓中，我们有两张表，一张是订单表 orders，另一张是用户表 users。显然，订单表是事实表（Fact），而用户表是维度表（Dimension）。业务需求是统计所有头部用户贡献的营业额，并按照营业额倒序排序。

```
// 订单表orders关键字段
userId, Int
itemId, Int
price, Float
quantity, Int

// 用户表users关键字段
id, Int
name, String
type, String //枚举值，分为头部用户和长尾用户
```

给定上述数据表，我们只需把两张表做内关联，然后分组、聚合、排序，就可以实现业务逻辑，具体的查询语句如下。

```sql
select (orders.price * order.quantity) as income, users.name
from orders
inner join users on orders.userId = users.id
where users.type = 'Head User'
group by users.name
order by income desc
```

不难发现，**如果过滤谓词中包含分区键，那么 Spark SQL 对分区表做扫描的时候，是完全可以跳过（剪掉）不满足谓词条件的分区目录，这就是分区剪裁**。例如，在我们的查询语句中，用户表的过滤谓词是 `users.type = 'Head User'`。假设用户表是分区表，那么对于用户表的数据扫描，Spark SQL 可以完全跳过前缀为"Tail User"的子目录。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202312140331492.png)

通过与谓词下推作对比，我们可以直观地感受分区剪裁的威力。如图所示，上下两行分别表示用户表在不做分区和做分区的情况下，Spark SQL 对于用户表的数据扫描。在不做分区的情况下，用户表所有的数据分片全部存于同一个文件系统目录，尽管 Parquet 格式在注脚（Footer）中提供了 type 字段的统计值，Spark SQL 可以利用谓词下推来减少需要扫描的数据分片，但由于很多分片注脚中的 type 字段同时包含‘Head User’和‘Tail User’（第一行 3 个浅绿色的数据分片），因此，用户表的数据扫描仍然会涉及 4 个数据分片。

相反，当用户表本身就是分区表时，由于 type 字段为‘Head User’的数据记录全部存储到前缀为‘Head User’的子目录，也就是图中第二行浅绿色的文件系统目录，这个目录中仅包含两个 type 字段全部为‘Head User’的数据分片。这样一来，Spark SQL 可以完全跳过其他子目录的扫描，从而大幅提升 I/O 效率。

---

### 实现原理

我们刚才说了，**DPP 指的是在数据关联的场景中，Spark SQL 利用维度表提供的过滤信息，减少事实表中数据的扫描量、降低 I/O 开销，从而提升执行性能**。那么，DPP 是怎么做到这一点的呢？它背后的逻辑是什么？

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202312140333962.png)

首先，过滤条件 users.type = ‘Head User’会帮助维度表过滤一部分数据。与此同时，维度表的 ID 字段也顺带着经过一轮筛选，如图中的步骤 1 所示。经过这一轮筛选之后，保留下来的 ID 值，仅仅是维度表 ID 全集的一个子集。

然后，在关联关系也就是 orders.userId = users.id 的作用下，过滤效果会通过 users 的 ID 字段传导到事实表的 userId 字段，也就是图中的步骤 2。这样一来，满足关联关系的 userId 值，也是事实表 userId 全集中的一个子集。把满足条件的 userId 作为过滤条件，应用（Apply）到事实表的数据源，就可以做到减少数据扫描量，提升 I/O 效率，如图中的步骤 3 所示。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202312140334201.png)

虽然 DPP 的运作逻辑非常清晰，但并不是所有的数据关联场景都可以享受到 DPP 的优化机制，想要利用 DPP 来加速事实表数据的读取和访问，数据关联场景还要满足三个额外的条件。

首先，DPP 是一种分区剪裁机制，它是以分区为单位对事实表进行过滤。结合刚才的逻辑，维度表上的过滤条件会转化为事实表上 Join Key 的过滤条件。具体到我们的例子中，就是 orders.userId 这个字段。显然，DPP 生效的前提是事实表按照 orders.userId 这一列预先做好了分区。因此，**事实表必须是分区表，而且分区字段（可以是多个）必须包含 Join Key**。

其次，过滤效果的传导，依赖的是等值的关联关系，比如 orders.userId = users.id。因此，**DPP 仅支持等值 Joins，不支持大于、小于这种不等值关联关系**。

此外，DPP 机制得以实施还有一个隐含的条件：**维度表过滤之后的数据集要小于广播阈值**。

---

结合刚才对于 DPP 实现逻辑的分析和推导，我们不难发现，实现 DPP 机制的关键在于，我们要让处理事实表的计算分支，能够拿到满足过滤条件的 Join Key 列表，然后用这个列表来对事实表做分区剪裁。那么问题来了，用什么办法才能拿到这个列表呢？

Spark SQL 选择了一种“一箭双雕”的做法：**使用广播变量封装过滤之后的维度表数据**。具体来说，在维度表做完过滤之后，Spark SQL 在其上构建哈希表（Hash Table），这个哈希表的 Key 就是用于关联的 Join Key。在我们的例子中，Key 就是满足过滤 users.type = ‘Head User’条件的 users.id；Value 是投影中需要引用的数据列，在之前订单表与用户表的查询中，这里的引用列就是 users.name。

哈希表构建完毕之后，Spark SQL 将其封装到广播变量中，这个广播变量的作用有二。第一个作用就是给事实表用来做分区剪裁，如图中的步骤 1 所示，哈希表中的 Key Set 刚好可以用来给事实表过滤符合条件的数据分区。

第二个作用就是参与后续的 Broadcast Join 数据关联，如图中的步骤 2 所示。这里的哈希表，本质上就是 Hash Join 中的 Build Table，其中的 Key、Value，记录着数据关联中所需的所有字段，如 users.id、users.name，刚好拿来和事实表做 Broadcast Hash Join。

---

### 总结

相比于谓词下推，分区剪裁往往能更好地提升磁盘访问的 I/O 效率。

这是因为，谓词下推操作往往是根据文件注脚中的统计信息完成对文件的过滤，过滤效果取决于文件中内容的“纯度”。分区剪裁则不同，它的分区表可以把包含不同内容的文件，隔离到不同的文件系统目录下。这样一来，包含分区键的过滤条件能够以文件系统目录为粒度对磁盘文件进行过滤，从而大幅提升磁盘访问的 I/O 效率。

而动态分区剪裁这个功能主要用在星型模型数仓的数据关联场景中，它指的是在运行的时候，Spark SQL 利用维度表提供的过滤信息，来减少事实表中数据的扫描量、降低 I/O 开销，从而提升执行性能。

动态分区剪裁运作的背后逻辑，是把维度表中的过滤条件，通过关联关系传导到事实表，来完成事实表的优化。在数据关联的场景中，开发者要想利用好动态分区剪裁特性，需要注意 3 点：

- 事实表必须是分区表，并且分区字段必须包含 Join Key
- 动态分区剪裁只支持等值 Joins，不支持大于、小于这种不等值关联关系
- 维度表过滤之后的数据集，必须要小于广播阈值，因此，开发者要注意调整配置项 `spark.sql.autoBroadcastJoinThreshold`
