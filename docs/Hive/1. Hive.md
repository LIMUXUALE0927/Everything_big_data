## Hive 简介

**Hadoop 解决了大数据存储和计算的问题，但是 MapReduce 编程不方便，HDFS 上的文件没有 Schema，统计分析比较困难，为了解决这个问题，产生了 Hive。**

Hive 是由 Facebook 开源，基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。

Hive 本质：是一个 Hadoop 客户端，用于将 HQL (Hive SQL) 转化成 MapReduce 程序。

---

## Hive 和数据库的比较

- **数据存储位置：**

Hive 是建立在 Hadoop 之上的。所有 Hive 的数据都是存储在 HDFS 中的；而数据库则可以将数据保存在块设备或者本地文件系统中。

- **索引：**

Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。值得一提的是，Hive 在 0.8 版本之后引入了图索引。

**在数据库中， 通常会针对一列或者几列建立索引，因此对于少量的、特定条件的数据的访问， 数据库可以有很高的效率和较低的延迟**。

由于数据的访问延迟较高，决定了 **Hive 不适合在线数据查询**。

- **数据格式：**

Hive 中没有定义专门的数据格式，数据格式可以由用户指定。用户定义数据格式需要指定三个属性: 列分隔符、行分隔符及读取文件数据的方法 (Hive 中默认有三种文件格式:TextFile、SequenceFile 及 RCFile)。由于在加载数据的过程中不需要从用户定义的数据格式到 Hive 定义的数据格式的转换，因此，**Hive 在加载过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中**。

而在数据库中，不同的数据库有不同的存储引擎，而且定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。

- **执行：**

Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的 (类似于 `select * from` 的查询不需要 MapReduce)，而数据库通常有自己的执行引擎。

- **数据更新：**

由于 Hive 是针对数据仓库应用设计的，而**数据仓库的内容是读多写少的**。因此，**Hive 中不建议对数据的改写**，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的。

- **执行延迟：**

Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。 当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候， Hive 的并行计算显然能体现出优势。

- **可扩展性：**

由于 Hive 是建立在 Hadoop 之上的。因此 Hive 的可扩展性和 Hadoop 的可扩展性是一致的；而数据库由于 ACID 语义的严格限制，扩展性非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。

- **数据规模：**

由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。

---

## Hive 架构组成

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310161743559.png)

由图中可以看出，Hive 是建立在 Hadoop 基础上的，是针对 Hadoop MapReduce 开发的技术。Hive 的组件包括 CLI(Command Line Interface)、JDBC/ODBC、Thrift Server、Web GUI、MetaStore 和 Driver(Compiler、Optimizer 和 Executor)。其实这么多组件大致可以分为两类：**客户端组件**和**服务端组件**。

**客户端组件：**

- CLI

Command Line Interface 命令行接口。最常用的客户端组件就是 CLI，CLI 启动 的时候，会同时启动一个 Hive 副本。

Client 是 Hive 的客户端，用于连接 HiveServer。在启动 Client 模式的时候， 需要指出 Hive Server 所在的节点，并且在该节点启动 Hive Server。图上所示的架构图里没有写上 Thrift 客户端，但是 Hive 架构的许多客户端接口都是建立在 Thrift 客户端之上的，包括 JDBC 和 ODBC 接口。

- Web GUI

Hive 客户端提供了一种通过网页访问 Hive 所提供的服务的方式。这个接口对 应 Hive 的 HWI(Hive Web Interface)组件，使用前要启动 HWI 服务。

**服务端组件：**

- Driver 组件

该组件包括 Compiler、Optimizer 和 Executor，其作用是完成 HQL 查询语句的词法分析、语法分析、编译、优化及查询计划的生成。生成的查询计 划存储在 HDFS 中，并在随后由 MapReduce 调用执行。

- MetaStore 组件

元数据对于 Hive 十分重要，因此 Hive 支持把 MetaStore 服务独立出来，安装到 远程的服务器集群里从而解耦 Hive 服务和 MetaStore 服务，保证 Hive 运行的健 壮性。

Hive 的 **MetaStore 组件是 Hive 元数据的集中存放地**。MetaStore 组件包括两个部分：MetaStore 服务和后台数据的存储。

默认情况下，MetaStore 服务和 Hive 服务是安装在一起的，运行在同一个进 程当中，也可以把 MetaStore 服务从 Hive 服务中剥离出来独立安装在一个集群里， Hive 远程调用 MetaStore 服务。

我们可以把元数据这一层放到防火墙之后，当客户端访问 Hive 服务时就可以连接到元数据这一层从而提供更好的管理性能和安全保障。使用远程的 MetaStore 服务，可以让 MetaStore 服务和 Hive 服务运行在不同的进程里，这样既保证了 Hive 的稳定性，又提升了 Hive 服务的效率。

- Thrift 服务

Thrift 是 Facebook 开发的一个软件框架，它用来进行可扩展且跨语言服务的开发。Hive 集成了该服务，可以让不同的编程语言调用 Hive 的接口。

正常的 Hive 仅允许使用 HiveQL 执行查询、更新等操作，并且该方式比较笨拙单一。幸好 Hive 提供了轻客户端的实现，通过 HiveServer 或者 HiveServer2， 客户端可以在不启动 CLI 的情况下对 Hive 中的数据进行操作，两者都允许远程客户端使用多种编程语言如 Java、Python 向 Hive 提交请求、取回结果，使用 jdbc 协议连接 hive 的 thriftserver 服务器，它可以实现远程访问。

---

## Hive 底层执行架构

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170135043.png)

步骤 1：UI 调用 DRIVER 的接口;

步骤 2：DRIVER 为查询创建会话句柄，并将查询发送到 COMPILER(编译器)生成 执行计划;

步骤 3 和 4：编译器从元数据存储中获取本次查询所需要的元数据，该元数据用 于对查询树中的表达式进行类型检查，以及基于查询谓词修建分区;

步骤 5：编译器生成的计划是分阶段的 DAG，每个阶段要么是 map/reduce 作业， 要么是一个元数据或者 HDFS 上的操作。将生成的计划发给 DRIVER。如果是 map/reduce 作业，该计划包括 map operator trees 和一个 reduce operator tree，执行引擎将会把这些作业发送给 MapReduce。

步骤 6、6.1、6.2 和 6.3：执行引擎将这些阶段提交给适当的组件。在每个 task(mapper/reducer) 中，从 HDFS 文件中读取与表或中间输出相关联的数据， 并通过相关算子树传递这些数据。最终这些数据通过序列化器写入到一个临时 HDFS 文件中(如果不需要 reduce 阶段，则在 map 中操作)。临时文件用于向 计划中后面的 map/reduce 阶段提供数据。

步骤 7、8 和 9：最终的临时文件将移动到表的位置，确保不读取脏数据(文件重 命名在 HDFS 中是原子操作)。对于用户的查询，临时文件的内容由执行引擎直接 从 HDFS 读取，然后通过 Driver 发送到 UI。

---

## Hive SQL 编译过程

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170140969.png)

编译 SQL 的任务是在上节中介绍的 COMPILER(编译器组件)中完成的。Hive 将 SQL 转化为 MapReduce 任务，整个编译过程分为六个阶段:

1.  解析器（SQLParser）：**词法、语法解析**，将 SQL 字符串转换成抽象语法树（AST）
2.  **语义解析**（Semantic Analyzer）：将 AST 进一步划分为一个个查询单元（QueryBlock），并将 Hive 中的元信息赋给每个 QueryBlock
3.  逻辑计划生成器（Logical Plan Gen）：通过语法树**生成逻辑执行计划**（OperatorTree）
4.  逻辑优化器（Logical Optimizer）：**对逻辑计划进行优化**（对 OperatorTree 变换，合并 Operator，达到减少传输数据量）
5.  物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划**生成物理计划**（遍历 OperatorTree，翻译为 MR 任务）
6.  物理优化器（Physical Optimizer）：**对物理计划进行优化**（对 MR 任务的变换，如 Map Join）

注：

- 词法、语按照 Antlr 定义的 SQL 语法规则完成解析
- 词法解析：识别关键字（如 SELECT），生成一个个的 TOKEN
- 语法解析：对一系列的 TOKEN 根据预设规则生成一个个的短句（如把 WHERE 过滤条件转换为一个表达式），再把一系列短句组合成一个完整的语法结构（树状结构 AST）
- 语义解析：遍历 AST，抽象出查询的基本组成单元 QueryBlock。Hive 调用 MetastoreClient 接口，将元信息注入符号解析空间，再将 AST 抽象成 QueryBlock，一个 QueryBlock 是一条 SQL 最基本的组成单元，包含输入源、计算过程、输出。
- 逻辑执行计划实际上是由一个逻辑操作表达树（Logical Operator Tree）表达的，Logical Operator Tree 是由一系列的 Operator 组成的（如 SELECT Operator, JOIN Operator）
- 逻辑计划优化包括：投影剪切（去掉不需要的列）；谓词下推；将 Select-Select、Filter-Filter 合并为单个操作；多路 Join 等

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110258864.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110303920.png)

---

## 为什么 MapReduce 的执行效率很低？

MapReduce 的执行效率很低，这种低效是由它的执行模式决定的，所有的 MapTask、ReduceTask 全部是以进程的方式执行的，要启动进程、销毁进程，即使可以开启 JVM 重用，但是也是用的时候开启，结束之后关闭，而且 JVM 成本很高。

随着时代的发展，人们开发出计算处理能力更强大的数据处理工具，如 Spark、 Tez 等，因此有:Hive on MapReduce、Hive on Spark、Hive on Tez

所以 Hive 底层所支持的执行引擎有 MapReduce、Spark、Tez。

[开源 SQL-on-Hadoop 系统一览 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56782207)

---

## Hive 的使用

- 直接执行：`hive -e 语句`
- 脚本执行：`hive -f 脚本文件`
- 通过客户端执行

---

## MetaStore 内嵌模式、本地模式、远程模式

- Embedded metastore：内嵌模式

其特点是：hive 服务和 metastore 服务运行在同一个进程中，derby 服务也运行在该进程中。该模式无需特殊配置。

- Local metastore：本地模式

其特点是：hive 服务和 metastore 服务运行在同一个进程中，mysql 是单独的进程，可以在同一台机器上，也可以在远程机器上。该模式只需将 `hive-site.xml` 中的 ConnectionURL 指向 mysql，并配置好驱动名、数据库连接账号即可。

- Remote metastore：远程模式

其特点是：hive 服务和 metastore 在不同的进程内，可能是不同的机器。 该模式需要将 `hive.metastore.local` 设置为 false，并将 `hive.metastore.uris` 设置为 metastore 服务器 URI，如有多个 metastore 服务器，URI 之间用逗号分隔。metastore 服务器 URI 的格式为 thrift://host:port。

---

## Hive 元数据数据库包含的具体内容

- 是什么：本质上是用来存储 hive 中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。

- 做什么：主要用作数据管理，包括查看数据表之间的血缘关系、查看数据存储、数据表的访问权限控制等。

常见表：

- DBS：该表存储 Hive 中所有数据库的基本信息
- DATABASE_PARAMS：该表存储数据库的相关参数（创建数据库时的 WITH DBPROPERTIES 中的内容就存在这）
- TBLS：该表中存储 Hive 表、视图、索引表的基本信息
- SERDES：该表存储序列化使用的类信息

---

## Hive 的四种数据模型

Hive 的四种数据模型分别为 Database, Table, Partition 和 Bucket。

- Database

Database 相当于关系数据库里的命名空间(namespace)，它的作用是将用户和数据库的应用隔离到不同的数据库或模式中。

- Table

Hive 里的 Table 有两种类型：一种叫**内部表**，这种表的数据文件存储在 Hive 的数据仓库里；另一种叫**外部表**，这种表的数据文件可以存放在 Hive 数据仓库外部的分布式文件系统上，也可以放到 Hive 数据仓库里(注意：Hive 的数据仓库也就是 HDFS 上的一个目录，这个目录是 Hive 数据文件存储的默认路径，它可以在 Hive 的配置文件里进行配置，最终也会存放到元数据库里)。

内部表：Hive 的内部表和数据库中的 Table 在概念上是类似的，每张 Table 在 Hive 中都有一个相应的目录存储数据。Table 逻辑上由存储的数据和描述表格 中的数据形式的相关元数据组成。表存储的数据存放在分布式文件系统里，例如 HDFS，元数据存储在关系数据库里。

外部表：在创建外部表时。在 table 之前要加关键字 external，同时还要用 location 命令指定文件存储的路径。如果不使用 locaction 命令，数据文件也会放置到 Hive 的数据仓库里。**外部表指向已经在 HDFS 中存在的数据**，可以创建 Partition。(它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。**内部表的创建过程和数据加载过程可以独立完成，也可以在同一条语句中完成。在加载数据的过程中，实际数据会被移动到数据仓库目录中，之后对数据的访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。**)而**外部表只有一个过程，加载数据和创建表同时完成(CREATE EXTERNAL TABLE... LOCATION)，实际数据存储在 LOCATION 后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。**

区别：**内部表在执行 drop 命令的时候，会删除元数据和存储的数据；而外部表在执行 drop 命令的时候，只删除元数据库里的数据，而不会删除存储的数据。**

- Partition

定义：Hive 里 Partition 的概念是根据「分区列」的值对表的数据进行粗略划分的机制。在 Hive 存储上就体现在表的主目录(Hive 的表实际显示就是一个文件夹)下的一个子目录，这个文件夹的名字就是我们定义的分区列的名字。分区列不是表的某个字段，而是独立的列，我们根据这个列来存 储表中的数据文件。

作用：使用分区是为了加快数据的查询速度。我们在查询某个具体分区列里的数据时，没必要进行全表扫描。

- Bucket

Table 和 Partition 都是目录级别的拆分数据，而 Bucket 则是**针对数据源数据文件本身来拆分数据**的。使用桶的表会将源数据文件按一定规律拆分成多个文件。桶运用的场景有限，一个是做 Map 连接的运算，另外一个就是取样操作。

Bucket 是将表的列通过 Hash 算法进一步分解成不同的文件存储。它对指定列计算 Hash 根据 Hash 值切分数据，目的是为了并行，每个 Bucket 对应一个文件。

---

## Hive 数据类型

| 基本数据类型 | 说明                                                 | 定义          |
| ------------ | ---------------------------------------------------- | ------------- |
| tinyint      | 1byte 有符号整数                                     |               |
| smallint     | 2byte 有符号整数                                     |               |
| **int**      | 4byte 有符号整数                                     |               |
| **bigint**   | 8byte 有符号整数                                     |               |
| boolean      | 布尔类型，true 或者 false                            |               |
| float        | 单精度浮点数                                         |               |
| **double**   | 双精度浮点数                                         |               |
| **decimal**  | 十进制精准数字类型                                   | decimal(16,2) |
| **varchar**  | 字符序列，需指定最大长度，最大长度的范围是 [1,65535] | varchar(32)   |
| **string**   | 字符串，无需指定最大长度                             |               |
| timestamp    | 时间类型                                             |               |
| binary       | 二进制数据                                           |               |

集合数据类型如下：

| 类型   | 说明                           | 定义                          | 取值         |
| ------ | ------------------------------ | ----------------------------- | ------------ |
| array  | 数组是一组相同类型的值的集合   | `array<string>`               | `arr[0]`     |
| map    | map 是一组相同类型的键值对集合 | `map<string, int>`            | `map['key']` |
| struct | 结构体                         | `struct<id:int, name:string>` | `struct.id`  |

> 通过集合类型来定义列的好处是什么?

在大数据系统中，不遵循标准格式的一个好处就是可以**提供更高吞吐量**的数据。当处理的数据的数量级是 T 或者 P 时，以最少的「头部寻址」来从磁盘上扫描 数据是非常必要的。**按数据集进行封装的话可以通过减少寻址次数来提供查询的速度**。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。

Hive 的基本数据类型可以做**类型转换**，转换的方式包括隐式转换以及显式转换。

隐式转换：

- 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint
- 所有整数类型、float 和 **string** 类型都可以隐式地转换成 double
- tinyint、smallint、int 都可以转换为 float
- boolean 类型不可以转换为任何其它的类型
