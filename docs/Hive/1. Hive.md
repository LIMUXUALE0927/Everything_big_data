# Hive

## Hive 简介

**Hadoop 解决了大数据存储和计算的问题，但是 MapReduce 编程不方便，HDFS 上的文件没有 Schema，统计分析比较困难，为了解决这个问题，产生了 Hive。**

Hive 是由 Facebook 开源，基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。

Hive 本质：是一个 Hadoop 客户端，用于将 HQL (Hive SQL) 转化成 MapReduce 程序。

---

## Hive 和数据库的比较

- **数据存储位置：**

Hive 是建立在 Hadoop 之上的。所有 Hive 的数据都是存储在 HDFS 中的；而数据库则可以将数据保存在块设备或者本地文件系统中。

- **索引：**

Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。值得一提的是，Hive 在 0.8 版本之后引入了图索引。

**在数据库中， 通常会针对一列或者几列建立索引，因此对于少量的、特定条件的数据的访问， 数据库可以有很高的效率和较低的延迟**。

由于数据的访问延迟较高，决定了 **Hive 不适合在线数据查询**。

- **数据格式：**

Hive 中没有定义专门的数据格式，数据格式可以由用户指定。用户定义数据格式需要指定三个属性: 列分隔符、行分隔符及读取文件数据的方法 (Hive 中默认有三种文件格式:TextFile、SequenceFile 及 RCFile)。由于在加载数据的过程中不需要从用户定义的数据格式到 Hive 定义的数据格式的转换，因此，**Hive 在加载过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中**。

而在数据库中，不同的数据库有不同的存储引擎，而且定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。

- **执行：**

Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的 (类似于 `select * from` 的查询不需要 MapReduce)，而数据库通常有自己的执行引擎。

- **数据更新：**

由于 Hive 是针对数据仓库应用设计的，而**数据仓库的内容是读多写少的**。因此，**Hive 中不建议对数据的改写**，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的。

- **执行延迟：**

Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。 当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候， Hive 的并行计算显然能体现出优势。

- **可扩展性：**

由于 Hive 是建立在 Hadoop 之上的。因此 Hive 的可扩展性和 Hadoop 的可扩展性是一致的；而数据库由于 ACID 语义的严格限制，扩展性非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。

- **数据规模：**

由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。

---

## Hive 架构组成

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310161743559.png)

由图中可以看出，Hive 是建立在 Hadoop 基础上的，是针对 Hadoop MapReduce 开发的技术。Hive 的组件包括 CLI(Command Line Interface)、JDBC/ODBC、Thrift Server、Web GUI、MetaStore 和 Driver(Compiler、Optimizer 和 Executor)。其实这么多组件大致可以分为两类：**客户端组件**和**服务端组件**。

**客户端组件：**

- CLI

Command Line Interface 命令行接口。最常用的客户端组件就是 CLI，CLI 启动的时候，会同时启动一个 Hive 副本。

Client 是 Hive 的客户端，用于连接 HiveServer。在启动 Client 模式的时候， 需要指出 Hive Server 所在的节点，并且在该节点启动 Hive Server。图上所示的架构图里没有写上 Thrift 客户端，但是 Hive 架构的许多客户端接口都是建立在 Thrift 客户端之上的，包括 JDBC 和 ODBC 接口。

- Web GUI

Hive 客户端提供了一种通过网页访问 Hive 所提供的服务的方式。这个接口对应 Hive 的 HWI(Hive Web Interface)组件，使用前要启动 HWI 服务。

**服务端组件：**

- Driver 组件

该组件包括 Compiler、Optimizer 和 Executor，其作用是完成 HQL 查询语句的词法分析、语法分析、编译、优化及查询计划的生成。生成的查询计 划存储在 HDFS 中，并在随后由 MapReduce 调用执行。

- MetaStore 组件

元数据对于 Hive 十分重要，因此 Hive 支持把 MetaStore 服务独立出来，安装到远程的服务器集群里从而解耦 Hive 服务和 MetaStore 服务，保证 Hive 运行的健壮性。

Hive 的 **MetaStore 组件是 Hive 元数据的集中存放地**。MetaStore 组件包括两个部分：MetaStore 服务和后台数据的存储。

默认情况下，MetaStore 服务和 Hive 服务是安装在一起的，运行在同一个进程当中，也可以把 MetaStore 服务从 Hive 服务中剥离出来独立安装在一个集群里， Hive 远程调用 MetaStore 服务。

我们可以把元数据这一层放到防火墙之后，当客户端访问 Hive 服务时就可以连接到元数据这一层从而提供更好的管理性能和安全保障。使用远程的 MetaStore 服务，可以让 MetaStore 服务和 Hive 服务运行在不同的进程里，这样既保证了 Hive 的稳定性，又提升了 Hive 服务的效率。

- Thrift 服务

Thrift 是 Facebook 开发的一个软件框架，它用来进行可扩展且跨语言服务的开发。Hive 集成了该服务，可以让不同的编程语言调用 Hive 的接口。

正常的 Hive 仅允许使用 HiveQL 执行查询、更新等操作，并且该方式比较笨拙单一。幸好 Hive 提供了轻客户端的实现，通过 HiveServer 或者 HiveServer2， 客户端可以在不启动 CLI 的情况下对 Hive 中的数据进行操作，两者都允许远程客户端使用多种编程语言如 Java、Python 向 Hive 提交请求、取回结果，使用 jdbc 协议连接 hive 的 thriftserver 服务器，它可以实现远程访问。

---

## Hive 底层执行架构

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170135043.png)

步骤 1：UI 调用 DRIVER 的接口;

步骤 2：DRIVER 为查询创建会话句柄，并将查询发送到 COMPILER(编译器)生成 执行计划;

步骤 3 和 4：编译器从元数据存储中获取本次查询所需要的元数据，该元数据用 于对查询树中的表达式进行类型检查，以及基于查询谓词修建分区;

步骤 5：编译器生成的计划是分阶段的 DAG，每个阶段要么是 map/reduce 作业， 要么是一个元数据或者 HDFS 上的操作。将生成的计划发给 DRIVER。如果是 map/reduce 作业，该计划包括 map operator trees 和一个 reduce operator tree，执行引擎将会把这些作业发送给 MapReduce。

步骤 6、6.1、6.2 和 6.3：执行引擎将这些阶段提交给适当的组件。在每个 task(mapper/reducer) 中，从 HDFS 文件中读取与表或中间输出相关联的数据， 并通过相关算子树传递这些数据。最终这些数据通过序列化器写入到一个临时 HDFS 文件中(如果不需要 reduce 阶段，则在 map 中操作)。临时文件用于向 计划中后面的 map/reduce 阶段提供数据。

步骤 7、8 和 9：最终的临时文件将移动到表的位置，确保不读取脏数据(文件重 命名在 HDFS 中是原子操作)。对于用户的查询，临时文件的内容由执行引擎直接 从 HDFS 读取，然后通过 Driver 发送到 UI。

---

## Hive SQL 编译过程

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170140969.png)

编译 SQL 的任务是在上节中介绍的 COMPILER(编译器组件)中完成的。Hive 将 SQL 转化为 MapReduce 任务，整个编译过程分为六个阶段:

1.  解析器（SQLParser）：**词法、语法解析**，将 SQL 字符串转换成抽象语法树（AST）
2.  **语义解析**（Semantic Analyzer）：将 AST 进一步划分为一个个查询单元（QueryBlock），并将 Hive 中的元信息赋给每个 QueryBlock
3.  逻辑计划生成器（Logical Plan Gen）：通过语法树**生成逻辑执行计划**（OperatorTree）
4.  逻辑优化器（Logical Optimizer）：**对逻辑计划进行优化**（对 OperatorTree 变换，合并 Operator，达到减少传输数据量）
5.  物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划**生成物理计划**（遍历 OperatorTree，翻译为 MR 任务）
6.  物理优化器（Physical Optimizer）：**对物理计划进行优化**（对 MR 任务的变换，如 Map Join）

注：

- 词法、语按照 Antlr 定义的 SQL 语法规则完成解析
- 词法解析：识别关键字（如 SELECT），生成一个个的 TOKEN
- 语法解析：对一系列的 TOKEN 根据预设规则生成一个个的短句（如把 WHERE 过滤条件转换为一个表达式），再把一系列短句组合成一个完整的语法结构（树状结构 AST）
- 语义解析：遍历 AST，抽象出查询的基本组成单元 QueryBlock。Hive 调用 MetastoreClient 接口，将元信息注入符号解析空间，再将 AST 抽象成 QueryBlock，一个 QueryBlock 是一条 SQL 最基本的组成单元，包含输入源、计算过程、输出。
- 逻辑执行计划实际上是由一个逻辑操作表达树（Logical Operator Tree）表达的，Logical Operator Tree 是由一系列的 Operator 组成的（如 SELECT Operator, JOIN Operator）
- 逻辑计划优化包括：投影剪切（去掉不需要的列）；谓词下推；将 Select-Select、Filter-Filter 合并为单个操作；多路 Join 等

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110258864.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110303920.png)

---

## 为什么 MapReduce 的执行效率很低？

MapReduce 的执行效率很低，这种低效是由它的执行模式决定的，所有的 MapTask、ReduceTask 全部是以进程的方式执行的，要启动进程、销毁进程，即使可以开启 JVM 重用，但是也是用的时候开启，结束之后关闭，而且 JVM 成本很高。

随着时代的发展，人们开发出计算处理能力更强大的数据处理工具，如 Spark、 Tez 等，所以 Hive 底层所支持的执行引擎有 MapReduce、Spark、Tez。

[开源 SQL-on-Hadoop 系统一览 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56782207)

---

## Hive 的使用

- 直接执行：`hive -e 语句`
- 脚本执行：`hive -f 脚本文件`
- 通过客户端执行

---

## MetaStore 内嵌模式、本地模式、远程模式

- Embedded metastore：内嵌模式

其特点是：hive 服务和 metastore 服务运行在同一个进程中，derby 服务也运行在该进程中。该模式无需特殊配置。

- Local metastore：本地模式

其特点是：hive 服务和 metastore 服务运行在同一个进程中，mysql 是单独的进程，可以在同一台机器上，也可以在远程机器上。该模式只需将 `hive-site.xml` 中的 ConnectionURL 指向 mysql，并配置好驱动名、数据库连接账号即可。

- Remote metastore：远程模式

其特点是：hive 服务和 metastore 在不同的进程内，可能是不同的机器。 该模式需要将 `hive.metastore.local` 设置为 false，并将 `hive.metastore.uris` 设置为 metastore 服务器 URI，如有多个 metastore 服务器，URI 之间用逗号分隔。metastore 服务器 URI 的格式为 thrift://host:port。

---

## Hive 元数据数据库包含的具体内容

- 是什么：本质上是用来存储 hive 中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。

- 做什么：主要用作数据管理，包括查看数据表之间的血缘关系、查看数据存储、数据表的访问权限控制等。

常见表：

- DBS：该表存储 Hive 中所有数据库的基本信息
- DATABASE_PARAMS：该表存储数据库的相关参数（创建数据库时的 WITH DBPROPERTIES 中的内容就存在这）
- TBLS：该表中存储 Hive 表、视图、索引表的基本信息
- SERDES：该表存储序列化使用的类信息

---

## Hive 的四种数据模型

Hive 的四种数据模型分别为 Database, Table, Partition 和 Bucket。

- Database

Database 相当于关系数据库里的命名空间(namespace)，它的作用是将用户和数据库的应用隔离到不同的数据库或模式中。

- Table

Hive 里的 Table 有两种类型：一种叫**内部表**，这种表的数据文件存储在 Hive 的数据仓库里；另一种叫**外部表**，这种表的数据文件可以存放在 Hive 数据仓库外部的分布式文件系统上，也可以放到 Hive 数据仓库里(注意：Hive 的数据仓库也就是 HDFS 上的一个目录，这个目录是 Hive 数据文件存储的默认路径，它可以在 Hive 的配置文件里进行配置，最终也会存放到元数据库里)。

内部表：Hive 的内部表和数据库中的 Table 在概念上是类似的，每张 Table 在 Hive 中都有一个相应的目录存储数据。Table 逻辑上由存储的数据和描述表格 中的数据形式的相关元数据组成。表存储的数据存放在分布式文件系统里，例如 HDFS，元数据存储在关系数据库里。

外部表：在创建外部表时。在 table 之前要加关键字 external，同时还要用 location 命令指定文件存储的路径。如果不使用 locaction 命令，数据文件也会放置到 Hive 的数据仓库里。**外部表指向已经在 HDFS 中存在的数据**，可以创建 Partition。(它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。**内部表的创建过程和数据加载过程可以独立完成，也可以在同一条语句中完成。在加载数据的过程中，实际数据会被移动到数据仓库目录中，之后对数据的访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。**)而**外部表只有一个过程，加载数据和创建表同时完成(CREATE EXTERNAL TABLE... LOCATION)，实际数据存储在 LOCATION 后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。**

区别：**内部表在执行 drop 命令的时候，会删除元数据和存储的数据；而外部表在执行 drop 命令的时候，只删除元数据库里的数据，而不会删除存储的数据。**

- Partition

定义：Hive 里 Partition 的概念是根据「分区列」的值对表的数据进行粗略划分的机制。在 Hive 存储上就体现在表的主目录(Hive 的表实际显示就是一个文件夹)下的一个子目录，这个文件夹的名字就是我们定义的分区列的名字。分区列不是表的某个字段，而是独立的列，我们根据这个列来存 储表中的数据文件。

作用：使用分区是为了加快数据的查询速度。我们在查询某个具体分区列里的数据时，没必要进行全表扫描。

- Bucket

Table 和 Partition 都是目录级别的拆分数据，而 Bucket 则是**针对数据源数据文件本身来拆分数据**的。使用桶的表会将源数据文件按一定规律拆分成多个文件。桶运用的场景有限，一个是做 Map 连接的运算，另外一个就是取样操作。

Bucket 是将表的列通过 Hash 算法进一步分解成不同的文件存储。它对指定列计算 Hash 根据 Hash 值切分数据，目的是为了并行，每个 Bucket 对应一个文件。

---

## Hive 数据类型

| 基本数据类型 | 说明                                                 | 定义          |
| ------------ | ---------------------------------------------------- | ------------- |
| tinyint      | 1byte 有符号整数                                     |               |
| smallint     | 2byte 有符号整数                                     |               |
| **int**      | 4byte 有符号整数                                     |               |
| **bigint**   | 8byte 有符号整数                                     |               |
| boolean      | 布尔类型，true 或者 false                            |               |
| float        | 单精度浮点数                                         |               |
| **double**   | 双精度浮点数                                         |               |
| **decimal**  | 十进制精准数字类型                                   | decimal(16,2) |
| **varchar**  | 字符序列，需指定最大长度，最大长度的范围是 [1,65535] | varchar(32)   |
| **string**   | 字符串，无需指定最大长度                             |               |
| timestamp    | 时间类型                                             |               |
| binary       | 二进制数据                                           |               |

集合数据类型如下：

| 类型   | 说明                           | 定义                          | 取值         |
| ------ | ------------------------------ | ----------------------------- | ------------ |
| array  | 数组是一组相同类型的值的集合   | `array<string>`               | `arr[0]`     |
| map    | map 是一组相同类型的键值对集合 | `map<string, int>`            | `map['key']` |
| struct | 结构体                         | `struct<id:int, name:string>` | `struct.id`  |

> 通过集合类型来定义列的好处是什么?

在大数据系统中，不遵循标准格式的一个好处就是可以**提供更高吞吐量**的数据。当处理的数据的数量级是 T 或者 P 时，以最少的「头部寻址」来从磁盘上扫描 数据是非常必要的。**按数据集进行封装的话可以通过减少寻址次数来提供查询的速度**。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。

> 集合类型的应用场景

map 类型常用来做扩展字段，避免多次修改表结构；而 array 类型常常出现在行列转化中。

Hive 的基本数据类型可以做**类型转换**，转换的方式包括隐式转换以及显式转换。

隐式转换：

- 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint
- 所有整数类型、float 和 **string** 类型都可以隐式地转换成 double
- tinyint、smallint、int 都可以转换为 float
- boolean 类型不可以转换为任何其它的类型

---

## Hive 序列化和反序列化

当进程在进行远程通信时，发送方需要把对象转化为字节序列才可在网络上传输，称为**对象序列化**；接收方则需要把字节序列恢复为对象， 称为对象的**反序列化**。

Hive 的反序列化是对 key/value 反序列化成 hive table 的每个列的值。 Hive 可以方便的将数据加载到表中而不需要对数据进行转换，这样在处理海量数据时可以节省大量的时间。

我们都知道，Hive 有三大核心组件：

- Query Processor：查询处理工具，对应源码中的 ql 包
- SerDe：序列化与反序列化器，源码 serde 包
- MetaStore：元数据存储及服务，源码 metastore 包

其中 SerDe 是 Serializer 和 Deserializer 的缩写，是序列化器与反序列化器，Hive 使用 SerDe 接口去完成 IO 操作，接口的三个主要功能：

- 序列化(Serialization)，从 Hive 写入 FS
- 反序列化（Deserialization），从 FS 读入 Hive
- 解释读写字段，加起文件到字段的映射作用

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191430557.png)

在 Hive 的 HQL 语句中，select 时将会用到反序列化操作， insert 时会用到序列化操作。

---

## 内部表和外部表

### 内部表与外部表的区别

内部表和外部表可以通过 `describe extended tablename;` 来查看。

Hive 中内部表与外部表的区别：

- 内部表数据由 Hive 自身管理，外部表数据由 HDFS 管理
- 内部表的数据存储位置是 `hive.metastore.warehouse.dir`，默认位置： `/user/hive/warehouse`，外部表数据的存储位置由自己指定（如果没有 LOCATION， Hive 将在 HDFS 上的 `/user/hive/warehouse` 文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）
- **删除内部表会直接删除元数据及存储数据，删除外部表仅仅会删除元数据，HDFS 上的文件并不会被删除**
- 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要通过修复（`MSCK REPAIR TABLE table_name;`）来进行同步元数据

需要注意的是**传统数据库对表数据验证是 schema on write（写时模式）**， 而 Hive 在 load 时是不检查数据是否符合 schema 的，**Hive 遵循的是 schema on read（读时模式）**，只有在读的时候 hive 才检查、解析具体的数据字段、schema。 读时模式的优势是加载数据非常迅速，因为它不需要读取数据进行解析，仅仅进行文件的复制或者移动。写时模式的优势是提升了查询性能，因为预先解析之后可以对列建立索引，并压缩，但这样也会花费要多的加载时间。

具体的使用场景：

- 做 etl 处理时，通常会选择内部表做中间表，因为清理时，会将 HDFS 上的文件同时删除。
- 如果怕误删数据，可以选择外部表，因为不会删除文件，方便恢复数据
- 如果对数据的处理都是通过 hql 语句完成，选择内部表，如果有其他工具一 同处理，选择外部表。
- 每天采集的 Nginx 日志和埋点日志，在存储的时候建议使用外部表，因为日志数据是采集程序实时采集进来的，一旦被误删，恢复起来非常麻烦。而且外部表方便数据的共享。
- 抽取过来的业务数据，其实用外部表或者内部表问题都不大，就算被误删，恢复起来也是很快的，如果需要对数据内容和元数据进行紧凑的管理，那还是建议使用内部表
- 在做统计分析时候用到的中间表，结果表可以使用内部表，因为这些数据不需要共享, 使用内部表更为合适。并且很多时候结果分区表我们只需要保留最近 3 天的数据，用外部表的时候删除分区时无法删除数据。

### 内部表与外部表的转换

如果创建的外部表在默认的路径下，不会删除文件，即使更改表名，该路径下的文件的名字也不会改变，此时如果再创建一个和原来名字相同的外部表，会造成两个表的数据路径是一样的。如果第二次创建的表和第一次创建的表结构不一致，在查询第二个表时，会报错。

**如果最开始创建了外部表在默认路径下，这个时候如果对表重命名的话，相应的路径名是不会变，这个时候如果在重建一个表名和原表一样名称的表就会造成两个表的数据路径是一样的，显然这是不想要的结果。**

- 首先创建一个外部表 `table_1`，但是路径不指定即为默认，这时候重命名 `table_1` 为 `table_111`，这个时候因为 `table_111` 为外部表，那他的文件路径仍然是 `/user/hive/warehouse/test.db/table_1`

- 如果这个时候再创建一个表 `table_1`，因为没有指定路径，`table_1` 数据路径也是 `/user/hive/warehouse/test.db/table_1`，而因为先后创建的两个表结构是一样，这个时候查询 `table_1`，其实也能查出原先 `table_1` 中的数据，这相当于 `table_1` 和 `table_111` 共用数据源。如果新建的第二个 `table_1` 表结构不一样的话，那查询 `table_1` 就会查询报查询结构的错

```sql
--- 内部表转外部表
alter table xm_testA set TBLPROPERTIES ("EXTERNAL"="TRUE")
--- 外部表转换内部表
alter table xm_testB set TBLPROPTIES ("EXTERNAL"="FALSE")
```

---

## 分区表与非分区表

**Hive 中的分区就是把一张大表的数据按照业务需要分散地存储到多个目录，每个目录就称为该表的一个分区**。在查询时通过 where 子句中的表达式选择查询所需要的分区，这样的查询效率会提高很多。

由于 Hive 实际是存储在 HDFS 上的抽象，Hive 的一个分区名对应一个目录名，子分区名就是子目录名，**并不是一个实际字段**。一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。

**分区是以字段的形式在表结构中存在**，可以把分区字段看作表的伪列，通过 `describe table` 命令可以查看到字段存在，**但是该字段不存放实际的数据内容**，仅仅是分区的表示。

### 静态分区和动态分区

hive 中支持两种类型的分区：静态分区 SP（static partition）、动态分区 DP （dynamic partition）。

动态分区是指向分区表 insert 数据时，被写入的分区不由用户指定，**而是由每行数据的最后一个字段的值来动态地决定**。**使用动态分区，可只用一个 insert 语句将数据写入多个分区**。

静态分区与动态分区的主要区别在于：**静态分区是手动指定，而动态分区是通过数据来进行判断**。详细来说，静态分区的列是在编译时期，通过用户传递来确定的；动态分区只有在 SQL 执行时才能确定。

写入的时候的区别：静态分区在插数语句的后边就要添加字段和字段值后，不需要在 sql 的查询语句再添加该字段；动态分区在插数语句后边添加字段后， 还需要在 sql 的查询语句的最后，把该字段加上。

静态分区：

```sql
insert into table test partition(dt='20210101')
select a, b, c
from source
where dt = '20210101';
--- test 字段：a, b, c, dt
```

动态分区：

```sql
insert into table test partition(dt)
select a, b, c, dt
from source
where dt >= '20210101';
--- 可以写入多个分区
```

---

## 分桶表

分桶的出现：分区提供了一个隔离数据和优化查询的便利方式，不过**并非所有的数据都可形成合理的分区**，尤其是需要确定合适大小的分区划分方式。

与分区的区别：分区表针对是目录，也就是存储路径，分桶表则针对的是文件，粒度更细，是另一种管理数据的方式。

数据分桶的原理: Hive 中按照分桶字段的 hash 值去模以分桶的个数。

**分桶的作用：**

- **提高查询效率**

全表查询或分区查询转换成桶查询。

- **大表 join**

对于两张大表的 join，执行 reduce join(shuffle join)肯定不合适，只能做 map join。但是 reduce join 只适合做小表和大表的 join，如何做大表间的 join 呢？此时可以**对两张大表的连接字段分桶**，此时的 join 是按照桶来的，字段值相同的 record 会进入同一个桶。

需要注意的是：两张大表的分桶数量上必须是倍数关系，确保对于连接条件的一致性。

解释：如果连接字段某一个值的 hashcode 为 8，A 表分 4 桶，B 表分 5 桶，则该 值在 A 表进入 0 号桶，在 B 表进入 3 号桶，此时 join 的连接条件就不一致了。 要想在 map 端执行桶 join，必须设置 `hive.optimize.bucketmapjoin=true`。

- **抽样查询**

传统抽样：`select * from emp tablesample(10%);`

Hive 提供了一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。

提示：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小 抽样单元是一个 HDFS 数据块。因此，如果表的数据大小小于普通的块大小 128M 的话，那么将会返回所有行。

分桶抽样：`select * from emp tablesample(bucket x out of y);`

y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。例如，table 总共分了 4 份（4 个 bucket），当 y=2 时，抽取(4/2=)2 个 bucket 的数据，当 y=8 时，抽取(4/8=)1/2 个 bucket 的数据。

x 表示从哪个 bucket 开始抽取。例如，table 总 bucket 数为 4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1 个 bucket 的数据，抽取第 4 个 bucket 的数据。

注意：x 的值必须小于等于 y 的值，否则会报错。

---

## 视图

Hive 中的视图（view）是一种虚拟表，只保存定义，不实际存储数据。通常从真实的物理表查询中创建生成视图，也可以从已经存在的视图上创建新视图。

创建视图时，将冻结视图的架构，如果删除或更改基础表，则视图将失败。概况起来就是：**视图是用来简化操作的，它其实是一张虚表，在视图中不缓冲记录，也没有提高查询性能。**

```sql
----【1】创建视图：
create view v_test as select customer_id, customer_city from olist_customers_dataset;
----【2】查看视图定义：
show create table v_test;
----【3】删除视图：
drop view v_test;
----【4】更改视图定义：
alter view v_test as select customer_id, customer_city, customer_state from olist _customers_dataset;
```

视图的作用：

- 将真实表中特定的列数据提供给用户，保护数据隐私。
- 通过视图降低查询的复杂度，优化查询语句，减少代码复杂度，提高可读性。

---

## 面试题

1. 请讲一下内部表与外部表的定义、区别、使用场景。
2. 请讲一下对分区、分桶的理解。
3. 请讲一下对静态分区、动态分区的理解。
4. 请讲一下如何处理大表 join 大表的问题。(按照关联字段分桶)

---

## Hive 中的四个 By

Hive 共有四个 by：order by，sort by，distribute by，cluster by。

**order by：**

- 全局排序
- 只有一个 reducer(多个 reducer 无法保证全局有序)；当输入规模较大时，需要较长的计算时间

**sort by：**

- 非全局排序
- 在数据进入 reducer 前完成排序
- 当 reducer 个数 > 1 时，只能保证每个 reducer 的输出有序，不保证全局有序

**distribute by：**

- 按照指定的字段对数据进行划分输出到不同的 reduce 中，后面不能跟 desc、 asc 排序
- 常和 sort by 一起使用，并且 distribute by 必须在 sort by 前面

**cluster by：**

- cluster by x 相当于 distribute by x + sort by x，只能默认升序，不能使用倒序

---

## Join

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191916980.png)

### Semi Join

[_SQL Basics: How To Use A LEFT SEMI JOIN and Why - YouTube_](https://www.youtube.com/watch?v=rcyhh_fjsc4)

[_Optimizing IN and EXISTS Subquery Predicates with Semijoin Transformations_](https://dev.mysql.com/doc/refman/8.0/en/semijoins.html)

mysql 中有一个语法 in/exists 用于判断字段是否在给定的值中。

对于 hql 语句去执行这个语句的时候需要转换为 MR 任务， hive 在 转换为 MR 任务的时候性能极低，这时候我们通用的解决方案就是用 join 解决。 `select * from a left semi join b on a.id=b.id;`

左半连接：判断左表中的关联键是否在右表中存在，若存在，则返回左表的存在的数据的相关字段，若不存在，则不返回。

```sql
select a.*
from user_base_info as a
left semi join user_phone_info as b
on a.user_id=b.user_id;
--等价于：
select a.*
from user_base_info as a
where a.user_id in (select user_id from user_phone_info);
--也等价于：
select a.*
from user_base_info as a
where EXISTS (select 1 from user_phone_info as b where a.user_id=b.user_id);
```

和普通 Join 的区别：a semi join b 只返回 a 表的数据，而 join 会返回 a 表 join b 表之后的全部数据。

### Common Join

如果不指定 Map Join 或者不符合 Map Join 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即在 Reduce 阶段完成 join。 整个过程包含 Map、Shuffle、Reduce 阶段。

**Map 阶段：**

- Step1：读取源表的数据，Map 输出时候以 Join on 条件中的列为 key，如果 Join 有多个关联键，则以这些关联键的组合作为 key
- Step2: Map 输出的 value 为 join 之后所关心的(select 或者 where 中需要用到的) 列；同时在 value 中还会包含表的 Tag 信息，用于标明此 value 对应哪个表
- Step3: 按照 key 进行排序。

**Shuffle 阶段：**

- 根据 key 的值进行 hash，并将 key/value 按照 hash 值推送至不同的 reduce 中，这样确保两个表中相同的 key 位于同一个 reduce 中。

**Reduce 阶段：**

- 根据 key 的值完成 join 操作，期间通过 Tag 来识别不同表中的数据。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170306655.png)

### Map Join

Map Join 通常用于一个很小的表和一个大表进行 join 的场景，具体小表有多小，由参数 `hive.mapjoin.smalltable.filesize` 来决定，该参数表示小表的总大小。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191954088.png)

Map Join 可以通过两个只有 map 阶段的 Job 完成一个 join 操作。其适用场景为大表 join 小表。若某 join 操作满足要求，则第一个 Job 会读取小表数据，将其制作为 hash table，并上传至 **Hadoop 分布式缓存** (本质上是上传至 HDFS)。第二个 Job 会先从分布式缓存中读取小表数据，并缓存在 MapTask 的内存中，然后扫描大表数据，这样在 map 端即可完成关联操作。

- Job1（本地任务）：读取小表数据、制作哈希表、上传至分布式缓存

- Job2：从分布式缓存读取哈希表、加载至内存、完成 join

触发 Map Join：

- 在 SQL 语句中增加 hint 提示（过时，不推荐使用）
- Hive 优化器根据表的数据量大小自动触发

hint 提示：将 ta 作为小表缓存

```sql
select /*+ mapjoin(ta) */
	ta.id,
	tb.id
from table_a ta
join table_b tb
on ta.id=tb.id;
```

自动触发：

Hive 在编译 SQL 语句阶段，起初所有的 Join 操作均采用 Common Join 算法实现。

之后在物理优化阶段，Hive 会根据每个 Common Join 任务所需表的大小判断该 Common Join 任务是否能够转换为 Map Join 任务，若满足要求，便将 Common Join 任务自动转换为 Map Join 任务。判断标准为小表大小与用户设置的阈值（MapTask 内存）比较。

但**有些 Common Join 任务所需的表大小，在 SQL 的编译阶段是未知的**（例如先进行一个分组聚合，之后再和一张表进行 Join 操作），所以这种 Common Join 任务是否能转换成 Map Join 任务在编译阶是无法确定的。

针对这种情况，Hive 会在编译阶段生成一个**条件任务 (Conditional Task)**，其下会包含一个**计划列表**，计划列表中包含**转换后的 Map Join 任务以及原有的 Common Join 任务**。 最终具体采用哪个计划，是在**运行时决定**的。大致思路如下图所示:

假设我们有 A 和 B 两张表，两张表的大小都未知，那么此时 Map Join 有 2 种情况：

- 缓存 A 表，扫描 B 表
- 缓存 B 表，扫描 A 表

因此会生成多种执行计划。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160539533.png)

---

### Bucket Map Join

Bucket Map Join 是**对 Map Join 的改进**，其打破了 Map Join 只适用于大表 join 小表的限制，可用于大表 join 大表的场景。

**分桶的原理：对分桶字段进行 hash partition，拆分为若干个文件。**

Bucket Map Join 的核心思想是：若能保证参与 join 的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与 join 的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行 Map Join 操作了。这样一来，第二个 Job 的 Map 端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160450333.png)

对于上面的图：

对于 Table B 来说，由于有 2 个桶，因此所有 `hashcode % 2 == 0` 的 record 都会进入 Bucket B-0，所有 `hashcode % 2 == 1` 的 record 都会进入 Bucket B-1。

对于 Table A 来说，由于有 4 个桶，因此所有 `hashcode % 4 == 0` 的 record 都会进入 Bucket A-0。所有 `hashcode % 4 == 2` 的 record 都会进入 Bucket A-2。Bucket A-0 和 Bucket A-2 中的这些 record 就对应着 Bucket B-0 中的 record。同理，Bucket A-1 和 Bucket A-3 中的所有 record 就对应着 Bucket B-1 中的 record。

当然，最简单的情况就是两张表的桶数相同。

之后就和 Map Join 的操作一致：

- Job1：先由本地 Map 任务将相对小一点的表的每个桶各制作一张哈希表，将所有哈希表上传至分布式缓存
- Job2：按照 BucketInputFormat 读取大表数据，一个桶一个切片，每一个 Mapper 只需要处理大表中的一个桶即可。同时，根据桶之间的对应关系，每个 Mapper 只需要从分布式缓存中读取自己需要的小表的桶的哈希表即可。例如，Mapper1 被分到了 Bucket A-0，因此，Mapper1 只需要去分布式缓存读取 Bucket B-0 即可。

---

### Sort Merge Bucket Map Join

Sort Merge Bucket Map Join(简称 SMB Map Join) 基于 Bucket Map Join。SMB Map Join 要求，参与 join 的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、 排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。

SMB Map Join 同 Bucket Join 一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行 join 操作，不同的是分桶之间的 join 操作的实现原理。Bucket Map Join，两个分桶之间的 join 实现原理为 **Hash Join 算法**；而 SMB Map Join，两个分桶之间的 join 实现原理为 **Sort Merge Join 算法**。

Hash Join 和 Sort Merge Join 均为关系型数据库中常见的 Join 实现算法。Hash Join 的原理相对简单，就是对参与 join 的一张表构建 hash table，然后扫描另外一张表， 然后进行逐行匹配。**Sort Merge Join 需要在两张按照关联字段排好序的表中进行**，其原理如图所示：

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160518467.png)

**SMB Map Join VS Bucket Map Join**

- 不需要制作哈希表
- 不需要在内存中缓存哈希表（一个桶），因此对桶的大小没有要求

Hive 中的 SMB Map Join 就是对两个分桶的数据按照上述思路进行 Join 操作。可以看出，SMB Map Join 与 Bucket Map Join 相比，在进行 Join 操作时，Map 端是无需对整个 Bucket 构建 hash table，也无需在 Map 端缓存整个 Bucket 数据的，每个 Mapper 只需按顺序逐个 key 读取两个分桶的数据进行 join 即可。

设置：

```sql
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
```

---

## 自定义函数

Hive 自定义函数包括三种 UDF、UDAF、UDTF。

- UDF(User-Defined-Function)：用户自定义函数，一对一的输入输出。
- UDAF(User-Defined Aggregation Funcation)：用户自定义聚合函数，多进一出。 例如：count/max/min。
- UDTF(User-Defined Table-Generating Functions)：用户自定义表生成函数，一对多的输入输出。例如：lateral view explode。

使用方式：

在 HIVE 会话中 add 自定义函数的 jar 文件，然后创建 function 继而使用函数。具体步骤如下：

1. 编写自定义函数
2. 打包上传到集群机器中
3. 进入 hive 客户端，添加 jar 包：`hive> add jar /home/hive_udf.jar`
4. 创建临时函数： `hive> create temporary function getLen as 'com.anson.GetLength';`
5. 使用临时函数： `hive> select getLen('1234567');`
6. 销毁临时函数：`hive> drop temporary function getLen;`

---

## 行式存储和列式存储

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231703893.png)

行式存储的特点：

**查询满足条件的一整行数据的时候**，列存储需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。

优点：具备快速数据加载和动态负载的高适应能力，因为**行存储保证了相同记录的所有域都在同一个集群节点**。

缺点：但是它不太满足快速的查询响应时间的要求，**特别是在当查询仅仅针对所有列中的少数几列时**，它就不能直接定位到所需列而跳过不需要的列，由于混合着不同数据值的列，行存储不易获得一个极高的压缩比（一行数据通常包含不同的数据类型）。

!!! note "行存储总结"

    需要查询一整行的数据的时候，行存储由于一行数据都聚集在一块，因此这种情况下查询更快。而如果仅仅查询少数几列数据时，则查询效率不高。同时，一行数据通常包含不同的数据类型，因此难以获得理想的压缩效果。

列式存储的特点：

因为**每个字段的数据聚集存储**，在查询只需要少数几个字段的时候，能大大减少读取的数据量。同时每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。

优点是：这种结构使得在查询时能够直接读取需要的列而避免不必要列的读取，并且对于相似数据也可以有一个更好的压缩比。

缺点是：它并不能提供基于 Hadoop 系统的快速查询处理，也不能保证同一记录的所有列都存储在同一集群节点之上，也不适应高度动态的数据负载模式。

Hive 中常用的存储格式有 TEXTFILE 、SEQUENCEFILE、AVRO、RCFILE、ORCFILE、 PARQUET 等，其中 TEXTFILE 、SEQUENCEFILE 和 AVRO 是行式存储，RCFILE、ORCFILE、 PARQUET 是列式存储。存储格式即是指表的数据是如何在 HDFS 上组织排列的。

---

## Hive 中的存储格式

Apache Hive 支持 Apache Hadoop 中使用的几种熟悉的文件格式，如 TextFile，RCFile，SequenceFile，AVRO，ORC 和 Parquet 格式。在建表时使用 STORED AS (TextFile|RCFile|SequenceFile|AVRO|ORC|Parquet) 来指定存储格式。

**TextFile** 每一行都是一条记录，每行都以换行符（\n）结尾。**数据不做压缩，磁盘开销大，数据解析开销大**。可结合 Gzip、Bzip2 使用（系统自动检查，执行查询时自动解压），但**使用这种方式，Hive 不会对数据进行切分，从而无法对数据进行并行操作**。

**SequenceFile** 是 Hadoop API 提供的一种**二进制文件**，它将数据**以 `<key,value>` 的形式序列化到文件中**。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空， 使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段的排序过程。**支持三种压缩**选择：NONE, RECORD, BLOCK。Record 压缩率低，一般建议使用 BLOCK 压缩。

**RCFile** 是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个 record 在一个块上，避免读一个记录需要读取多个 block。其次，块数据列式存储，有利于数据压缩和快速的列存取。

**ORC** 文件格式提供了一种将数据存储在 Hive 表中的高效方法。这个文件系统实际上是为了克服其他 Hive 文件格式的限制而设计的。Hive 从大型表读取，写入和处理数据时，使用 ORC 文件可以提高性能。

**Parquet** 是一个面向列的二进制文件格式。Parquet 对于大型查询的类型是高效的。对于扫描特定表格中的特定列的查询，Parquet 特别有用。Parquet 使用压缩 Snappy，gzip；目前 Snappy 默认。

**AVRO** 是开源项目，为 Hadoop 提供数据序列化和数据交换服务。Avro 是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro 能够提供更好的**序列化和反序列化性能**。

---

### RC File

RCFile（Record Columnar File）存储结构遵循的是「先水平划分，再垂直划分」 的设计理念。RCFile 结合了行存储和列存储的优点：首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231818368.png)

如上图是 HDFS 内 RCFile 的存储结构。每个 HDFS 块中，RCFile 以**行组**为基本单位来组织记录。对于一张表，所有行组大小都相同。一个 HDFS 块会有一个或多个行组。一个行组包括三个部分：

- 第一部分是行组头部的同步标识，主要用于分隔 HDFS 块中的两个连续行组；
- 第二部分是行组的元数据头部，用于存储行组单元的信息，包括行组中的记录数、每个列的字节数、列中每个域的字节数；
- 第三部分是表格数据段，即实际的列存储数据。

相比于其他列式存储的优势：

- 某些列式存储同一列可能存在不同的 block 上，在查询的时候，Hive 重组列的过程会浪费很多 IO 开销。而 RCFile 由于相同的列都是在一个 HDFS 块上，所以 相对列存储而言会节省很多资源。
- RCFile 采用**游程编码**，相同的数据不会重复存储，很大程度上节约了存储空间，尤其是字段中包含大量重复数据的时候。

注：游程编码是一种简单的压缩算法，如字符串「AAABBBCCC」可以用「3A3B3C」来表示。对于重复并且连续出现的数据压缩较为有效。

- RCFile 不支持任意方式的数据写操作，仅提供一种追加接口，这是因为底层的 HDFS 当前仅仅支持数据追加写文件尾部。
- 当处理一个行组时，RCFile 无需全部读取行组的全部内容到内存。相反，它仅仅读元数据头部和给定查询需要的列。因此，它**可以跳过不必要的列以获得列存储的 I/O 优势**。例如：`select c from table where a > 1`。

---

### ORC File

ORC File，它的全名是 Optimized Row Columnar (ORC) file，其实就是对 RCFile 做了一些优化。运用 ORC File 可以提高 Hive 的读、写以及处理数据的性能。

和 RCFile 格式相比，ORC File 格式有以下优点：

- 每个 task 只输出单个文件，这样可以减少 NameNode 的负载
- 支持各种复杂的数据类型，比如： datetime, decimal, 以及一些复杂类型 (struct, list, map, and union)
- 在文件中存储了一些轻量级的索引数据
- 基于数据类型的块模式压缩：integer 类型的列用游程编码 (run-length encoding)；String 类型的列用字典编码

ORC File 包含一组组的行数据，称为 stripe，除此之外，ORC File 的 file footer 还包含一些额外的辅助信息。在 ORC File 文件的最后，有一个被称为 postscript 的区，它主要是用来存储压缩参数及压缩页脚的大小。 在默认情况下，一个 stripe 的大小为 250MB。大尺寸的 stripe 使得从 HDFS 读数据更高效。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231834503.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231901999.png)

Index data 包含每列的最大和最小值以及每列所在的行。行索引里面提供了偏移量，它可以跳到正确的压缩块位置。具有相对频繁的行索引，使得在 stripe 中快速读取的过程中可以跳过很多行，尽管这个 stripe 的大小很大。在默认情况下，最大可以跳过 10000 行。拥有通过过滤谓词而跳过大量的行的能力。

- index data：每隔一万行的稀疏索引
- row data：实际数据
- stripe footer：数据的长度，类型等信息
- file footer：记录了各个 stripe 的信息，每个 stripe 中有多少行，以及每列的数据类型。当然，它里面还包含了列级别的一些聚合的结果， 比如：count, min, max, and sum
- postscript：记录了 file footer 的长度

使用 ORC File：

```sql
CREATE TABLE ... STORED AS ORC
ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
SET hive.default.fileformat=Orc
```

```sql
create table Addresses ( name string, street string, city string, state string, zip int )
stored as orc tblproperties ("orc.compress"="NONE")
```

---

### Parquet

关系数据库中使用数据模型通常都是扁平式的，但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能**需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的**，所以**如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果**。

Parquet 的灵感来自于 2010 年 Google 发表的 Dremel 论文，文中介绍了一种**支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能**。Parquet 仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是 parquet 相较于 ORC 的仅有优势：支持嵌套结构。Parquet 没有太多其他可圈可点的地方，比如他不支持 update 操作(数据写成后不可修改)，不支持 ACID 等。

Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231900429.png)

上图展示了一个 Parquet 文件的基本结构，文件的首尾都是该文件的 Magic Code，用于校验它是否是一个 Parquet 文件。

首尾中间由若干个 Row Group 和一个 Footer(File Meta Data) 组成。

每个 Row Group 包含多个 Column Chunk，每个 Column Chunk 包含多个 Page。以下是 Row Group、Column Chunk 和 Page 三个概念的说明:

- 行组 (Row Group)： 一个行组对应逻辑表中的若干行。
- 列块 (Column Chunk)： 一个行组中的一列保存在一个列块中。
- 页 (Page)：一个列块的数据会划分为若干个页。

Footer(File Meta Data) 中存储了每个行组 (Row Group) 中的每个列块 (Column Chunk) 的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的 Data Page 位置等信息。

除了支持嵌套结构，Parquet 还支持一些优化：

- 映射下推(Project PushDown)

说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现 TableScan 算子，而避免扫描整个表文件内容。

在 Parquet 中原生就支持映射下推，执行查询的时候可以通过 Configuration 传递需要读取的列的信息，映射每次会扫描一个 Row Group 的数据，然后一次性得将该 Row Group 里所有需要的列的 Column Chunk 都读取到内存中，每次读取一个 Row Group 的数据能够大大降低随机读的次数， 除此之外，Parquet 在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。

- 谓词下推(Predicate PushDown)

通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能。Parquet 做了更进一步的优化，优化的方法是对每一个 Row Group 的每一个 Column Chunk 在存储的时候都**计算对应的统计信息**，包括该 Column Chunk 的最大值、最小值和空值个数。**通过这些统计值和该列的过滤条件可以判断该 Row Group 是否需要扫描**。另外 Parquet 未来还会增加诸如 Bloom Filter 和 Index 等优化数据，更加有效的完成谓词下推。

在使用 Parquet 的时候可以通过如下两种策略提升查询性能：

- 类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。
- 减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的 I/O 负载。

**性能对比：**

相比传统的行式存储，Hadoop 生态圈近年来也涌现出诸如 RC、ORC、Parquet 的列式存储格式，它们的性能优势主要体现在两个方面：

1. 更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。
2. 更少的 I/O，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能。

在数据存储方面，ORC 和 Parquet 两种存储格式在都是用 snappy 压缩的情况 下两种存储格式占用的空间相差并不大，而且 Parquet 格式稍好于 ORC 格式。两者在功能上也都有优缺点，Parquet 原生支持嵌套式数据结构，而 ORC 对此支持较差，这种复杂的 Schema 查询也相对较差；而 Parquet 不支持数据的修改和 ACID，但是 ORC 对此提供支持，但是在 OLAP 环境下很少会对单条数据修改，更多的则是批量导入。

---

## 数据压缩

为了节省集群磁盘的存储资源，数据一般都是需要压缩的，目前在 Hadoop 中用的比较多的有 lzo、gzip、snappy、bzip2。

那么是否选择文件压缩呢？在 hadoop 作业执行过程中，job 执行速度更多的 是局限于 I/O，而不是受制于 CPU。如果是这样，通过文件压缩可以提高 hadoop 性能。然而，如果作业的执行速度受限于 CPU 的性能，那么压缩文件可能就不合适，因为文件的压缩和解压会花费掉较多的时间。（IO 密集任务还是 CPU 密集任务）

Hive 中的压缩：

- 中间结果的压缩

处理作业 map 任务和 reduce 任务之间的数据，对于中间压缩，最好选择一个节省 CPU 耗时的压缩方式。SnappyCodec 是一个较好的压缩格式，CPU 消耗较低。

- 输出结果的压缩

作业最终的输出也可以压缩，`hive.exec.compress.output` 这个属性控制这个操作。当然，如果仅仅只需要在某一次作业中使用最终压缩，那么，可 以直接在脚本中设置这个属性，而不必修改配置文件。

压缩格式：

- GZip
- BZip2
- Snappy
- LZO

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310232116184.png)

GZip 和 BZip2 压缩可以保证最小的压缩文件，但是过于消耗时间；Snappy 和 LZO 压缩和解压缩很快，但是压缩的文件较大。所以如何选择压缩格式，需要根据具体的需求决定。

### Gzip

优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop 本身支持，在应用中处理 gzip 格式的文件就和直接处理文本一样；有 hadoop native 库；大部分 linux 系统都自带 gzip 命令，使用方便。

缺点：不支持 split。

应用场景：当每个文件压缩之后在 130M 以内的（1 个块大小内），都可以考虑用 gzip 压缩格式。譬如说一天或者一个小时的日志压缩成一个 gzip 文件，运行 mapreduce 程序的时候通过多个 gzip 文件并行处理。hive 程序，streaming 程序，和 java 写的 mapreduce 程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。

### LZO

优点：压缩/解压速度也比较快，合理的压缩率；支持 split，是 hadoop 中最流行的压缩格式；支持 hadoop native 库；可以在 linux 系统下安装 lzop 命令，使用方便。

缺点：压缩率比 gzip 要低一些；hadoop 本身不支持，需要安装；在应用中对 lzo 格式的文件需要做一些特殊处理（为了支持 split 需要建索引，还需要指定 inputformat 为 lzo 格式）。

应用场景：一个很大的文本文件，压缩之后还大于 200M 以上的可以考虑，而且单个文件越大，lzo 优点越越明显。

### Snappy

优点：高速压缩速度和合理的压缩率；支持 hadoop native 库。

缺点：不支持 split；压缩率比 gzip 要低；hadoop 本身不支持，需要安装；linux 系统下没有对应的命令。

应用场景：当 mapreduce 作业的 map 输出的数据比较大的时候，作为 map 到 reduce 的中间数据的压缩格式；或者作为一个 mapreduce 作业的输出和另外一个 mapreduce 作业的输入。

### Bzip2

优点：支持 split；具有很高的压缩率，比 gzip 压缩率都高；hadoop 本身支持，但不支持 native；在 linux 系统下自带 bzip2 命令，使用方便。

缺点：压缩/解压速度慢；不支持 native。

应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为 mapreduce 作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持 split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。

---

## 拉链表

[漫谈数据仓库之拉链表（原理、设计以及在 Hive 中的实现）](https://cloud.tencent.com/developer/article/1752848)

拉链表是针对数据仓库设计中表存储数据的方式而定义的，主要 是维护历史状态，以及最新状态数据的一种表，拉链表根据拉链粒度的不同，实际上相当于快照，只不过做了优化，去除了一部分不变的记录，通过拉链表可以 很方便的还原出拉链时点的客户记录。

实现思路：拉链表专门用于解决在数据仓库中数据发生变化如何实现数据存储的问题，如果直接覆盖历史状态，会导致无法查询历史状态，如果将所有数据单独切片存储，会导致存储大量非更新数据的问题。**拉链表的设计是将更新的数据进行状态记录，没有发生更新的数据不进行状态存储**，用于存储所有数据在不同时间上的所有状态，**通过时间进行标记每个状态的生命周期**，查询时，根据需求可以获取指定时间范围状态的数据，默认用 9999-12-31 等最大值来表示最新状态。

适用场景：数据量有点大，表中某些字段有变化，但是变化的频率也不是很高，业务需求又需要统计这种变化状态，每天全量一份有点不太现实，不仅浪费了存储空间，有时可能业务统计也有点麻烦，这时，拉链表的作用就体现出来了，既节省空间，又满足了需求。

原始数据：

|     |            |            |      |
| --- | ---------- | ---------- | ---- |
| 1   | 2021-08-20 | 2021-08-20 | 创建 |
| 2   | 2021-08-20 | 2021-08-20 | 创建 |
| 3   | 2021-08-20 | 2021-08-20 | 创建 |
| 1   | 2021-08-20 | 2021-08-21 | 支付 |
| 2   | 2021-08-20 | 2021-08-21 | 完成 |
| 4   | 2021-08-21 | 2021-08-21 | 创建 |
| 1   | 2021-08-20 | 2021-08-22 | 完成 |
| 3   | 2021-08-20 | 2021-08-22 | 支付 |
| 4   | 2021-08-21 | 2021-08-22 | 支付 |
| 5   | 2021-08-22 | 2021-08-22 | 创建 |

拉链表：

|     |            |            |      |            |            |
| --- | ---------- | ---------- | ---- | ---------- | ---------- |
| 1   | 2021-08-20 | 2021-08-20 | 创建 | 2021-08-20 | 2021-08-21 |
| 1   | 2021-08-20 | 2021-08-21 | 支付 | 2021-08-21 | 2021-08-22 |
| 1   | 2021-08-20 | 2021-08-22 | 完成 | 2021-08-22 | 9999-12-31 |
| 2   | 2021-08-20 | 2021-08-20 | 创建 | 2021-08-20 | 2021-08-21 |
| 2   | 2021-08-20 | 2021-08-21 | 完成 | 2021-08-21 | 9999-12-31 |
| 3   | 2021-08-20 | 2021-08-20 | 创建 | 2021-08-20 | 2021-08-22 |
| 3   | 2021-08-20 | 2021-08-22 | 支付 | 2021-08-22 | 9999-12-31 |
| 4   | 2021-08-21 | 2021-08-21 | 创建 | 2021-08-21 | 2021-08-22 |
| 4   | 2021-08-21 | 2021-08-22 | 支付 | 2021-08-22 | 9999-12-31 |
| 5   | 2021-08-22 | 2021-08-22 | 创建 | 2021-08-22 | 9999-12-31 |

我们一般在数仓中通过增加 start_date, end_date 来表示数据的有效期。start_date 表示该条记录的生命周期开始时间，end_date 表示该条记录的生命周期结束时间；end_date = '9999-12-31' 表示该条记录目前处于有效状态；

如果查询当前所有有效的记录，则 `select * from order_his where end_date = '9999-12-31'`。
如果查询 2021-08-21 的历史快照，则 `select * from order_his where start_date <= '2021-08-21' and end_date >= '2021-08-21'`。

---

## Json 日志处理

### 多字段提取

数据源：

```
{"movie":"2791","rate":"4","time":"978302188","userid":"1"}
{"movie":"2687","rate":"3","time":"978824268","userid":"1"}
{"movie":"2018","rate":"4","time":"978301777","userid":"1"}
{"movie":"3105","rate":"5","time":"978301713","userid":"1"}
{"movie":"2797","rate":"4","time":"978302039","userid":"1"}
```

```sql
--解析多个字段
select json_tuple(loginfo,'movie','rate','time','userid') as (movie_id,rate,time,user_id)
from log_json
```

---

### 日志单字段解析

函数：`get_json_object(string json_string, string path)`

说明：

- 第一个参数填写 json 对象变量，第二个参数使用 `$` 表示 json 变量标识，然后用 `.` 或 `[]` 读取对象或数组。
- 如果输入的 json 字符串无效，那么返回 NULL。
- 每次只能返回一个数据项。

数据源：

```
{
"store":
        {
         "fruit":[{"weight":8,"type":"apple"}, {"weight":9,"type":"pear"}],
         "bicycle":{"price":19.95,"color":"red"}
         },
 "email":"amy@only_for_json_udf_test.net",
 "owner":"amy"
}
```

```sql
--1.get单层值：结果=amy
select get_json_object(loginfo,'$.owner') from data;
--2.get多层值：结果=19.95
select get_json_object(loginfo,'$.store.bicycle.price') from data;
--3.get 数组值[]：结果={"weight":8,"type":"apple"}
select get_json_object(loginfo,'$.store.fruit[0]') from data;
```

---

### URL 参数解析

```sql
---返回 facebook.com
select parse_url('http://facebook.com/path/p1.php?query=1','HOST');
---返回 /path/p1.php
select parse_url('http://facebook.com/path/p1.php?query=1','PATH');
---返回 query=1
select parse_url('http://facebook.com/path/p1.php?query=1','QUERY');
```
