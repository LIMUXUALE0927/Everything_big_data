# Hive 面试题

## Hive 概述

Hive 是由 Facebook 开源，**基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能**。

Hive 本质上是一个 Hadoop 客户端，用于将 HQL 转化成 MapReduce 程序。

Hive 最适合于静态数据分析，不需要快速给出结果，而且数据本身不会频繁变化。

Hive 本身不是一个完整的数据库，Hadoop 和 HDFS 的设计本身约束和局限性限制了 Hive 所能胜任的工作。其中最大的限制就是 Hive 不支持记录级别的更新、插入或者删除操作，只能通过查询生产新表或者将查询结果导入到文件中。同时，Hive 也不支持事务。

---

## Hive 和关系型数据库的区别

- **数据存储位置：**

Hive 是建立在 Hadoop 之上的。所有 Hive 的数据都是存储在 HDFS 中的；而数据库则可以将数据保存在块设备或者本地文件系统中。

- **索引：**

Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。值得一提的是，Hive 在 0.8 版本之后引入了 bitmap 索引。

**在数据库中， 通常会针对一列或者几列建立索引，因此对于少量的、特定条件的数据的访问，数据库可以有很高的效率和较低的延迟**。

由于数据的访问延迟较高，决定了 **Hive 不适合在线数据查询**。

- **数据格式：**

Hive 中没有定义专门的数据格式，数据格式可以由用户指定。用户定义数据格式需要指定三个属性: 列分隔符、行分隔符及读取文件数据的方法 (Hive 中默认有三种文件格式:TextFile、SequenceFile 及 RCFile)。由于在加载数据的过程中不需要从用户定义的数据格式到 Hive 定义的数据格式的转换，因此，**Hive 在加载过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中**。

而在数据库中，不同的数据库有不同的存储引擎，而且定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。

- **执行：**

Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的 (类似于 `select * from` 的查询不需要 MapReduce)，而数据库通常有自己的执行引擎。

- **数据更新：**

由于 Hive 是针对数据仓库应用设计的，而**数据仓库的内容是读多写少的**。因此，**Hive 中不建议对数据的改写**，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的。

- **执行延迟：**

Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。 当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候， Hive 的并行计算显然能体现出优势。

- **可扩展性：**

由于 Hive 是建立在 Hadoop 之上的。因此 Hive 的可扩展性和 Hadoop 的可扩展性是一致的；而数据库由于 ACID 语义的严格限制，扩展性非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。

- **数据规模：**

由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。

**OLTP VS OLAP**

- OLTP(Online Transaction Processing)：联机事务处理，主要用于处理数据库事务，适合对数据进行管理、更新和修改
- OLAP(Online Analytical Processing)：联机分析处理，主要用于对历史数据进行分析，适合对数据进行读和分析

---

## Hive 的组成

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310290348457.png)

客户端组件：

- CLI：通过命令行的方式运行 Hive 客户端
- Web UI：通过网页访问 Hive 提供的服务

服务端组件：

- Driver：包含解析器、编译器、优化器和执行器，作用是对 HQL 进行解析、编译、优化、生成执行计划，提交给底层的执行引擎
- MetaStore：存储元数据（表名、表所属数据库、表类型、表数据所在目录等），默认使用自带的 Derby 数据库，但缺点是无法并发运行 2 个 Hive CLI 实例，因此推荐使用 MySQL 数据库来存储元数据
- HiveServer2：服务端接口，通过 Thrift 协议提供对 Hive 的远程访问，我们可以利用 JDBC/ODBC 来访问 Hive，支持多客户端并发和身份验证

---

## Hive SQL 编译过程

深度好文：

[Hive SQL 的编译过程 - 美团技术团队 (meituan.com)](https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170140969.png)

Hive 将 SQL 转化为 MapReduce 任务，整个编译过程分为六个阶段:

1. 解析器（SQLParser）：**词法、语法解析**，将 SQL 字符串转换成抽象语法树（AST）
2. **语义解析**（Semantic Analyzer）：将 AST 进一步划分为一个个查询单元（QueryBlock），并将 Hive 中的元信息赋给每个 QueryBlock
3. 逻辑计划生成器（Logical Plan Gen）：通过语法树**生成逻辑执行计划**（OperatorTree）
4. 逻辑优化器（Logical Optimizer）：**对逻辑计划进行优化**（如谓词下推、投影剪切）
5. 物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划**生成物理计划**（遍历 OperatorTree，翻译为 MR 任务）
6. 物理优化器（Physical Optimizer）：**对物理计划进行优化**（对 MR 任务的变换，如 Map Join）

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110258864.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110303920.png)

---

## Hive 工作原理/执行流程

1. 用户提交查询任务给 Driver
2. Driver 将查询发送到编译器以生成执行计划
3. 编译器从 MetaStore 中获取所需要的元数据信息
4. 编译器对 HQL 进行编译，会经历语法解析（AST）、语义解析（QueryBlock）、生成逻辑计划、优化逻辑计划、生成物理计划、优化物理计划等过程
5. 编译器将最终的执行计划返回给 Driver
6. Driver 将执行计划提交给执行引擎（Execution Engine）去执行， 执行引擎将 MR 任务提交给 Job Tracker
7. 任务执行完后将结果返回给 Driver 并返回给用户

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310290348373.png)

---

## 为什么 MapReduce 的执行效率很低？

MapReduce 的执行效率很低，这种低效是由它的执行模式决定的，所有的 MapTask、ReduceTask 全部是以进程的方式执行的，要启动进程、销毁进程，即使可以开启 JVM 重用，但是也是用的时候开启，结束之后关闭，而且 JVM 成本很高。

随着时代的发展，人们开发出计算处理能力更强大的数据处理工具，如 Spark、 Tez 等，所以 Hive 底层所支持的执行引擎有 MapReduce、Spark、Tez。

[开源 SQL-on-Hadoop 系统一览 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56782207)

---

## MetaStore 内嵌模式、本地模式、远程模式

- Embedded metastore：内嵌模式

其特点是：**hive 服务和 metastore 服务运行在同一个进程中，derby 服务也运行在该进程中**。该模式无需特殊配置。

- Local metastore：本地模式

其特点是：**hive 服务和 metastore 服务运行在同一个进程中，mysql 是单独的进程**，可以在同一台机器上，也可以在远程机器上。该模式只需将 `hive-site.xml` 中的 ConnectionURL 指向 mysql，并配置好驱动名、数据库连接账号即可。

- Remote metastore：远程模式

其特点是：**hive 服务和 metastore 在不同的进程内，可能是不同的机器**。 该模式需要将 `hive.metastore.local` 设置为 false，并将 `hive.metastore.uris` 设置为 metastore 服务器 URI，如有多个 metastore 服务器，URI 之间用逗号分隔。metastore 服务器 URI 的格式为 thrift://host:port。

---

## Hive 元数据数据库包含的具体内容

- 是什么：本质上是用来存储 hive 中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。

- 做什么：主要用作数据管理，包括查看数据表之间的血缘关系、查看数据存储、数据表的访问权限控制等。

常见表：

- DBS：该表存储 Hive 中所有数据库的基本信息
- DATABASE_PARAMS：该表存储数据库的相关参数（创建数据库时的 WITH DBPROPERTIES 中的内容就存在这）
- TBLS：该表中存储 Hive 表、视图、索引表的基本信息
- SERDES：该表存储序列化使用的类信息

---

## Hive 数据类型

| 基本数据类型 | 说明                                                 | 定义          |
| ------------ | ---------------------------------------------------- | ------------- |
| tinyint      | 1byte 有符号整数                                     |               |
| smallint     | 2byte 有符号整数                                     |               |
| **int**      | 4byte 有符号整数                                     |               |
| **bigint**   | 8byte 有符号整数                                     |               |
| boolean      | 布尔类型，true 或者 false                            |               |
| float        | 单精度浮点数                                         |               |
| **double**   | 双精度浮点数                                         |               |
| **decimal**  | 十进制精准数字类型                                   | decimal(16,2) |
| **varchar**  | 字符序列，需指定最大长度，最大长度的范围是 [1,65535] | varchar(32)   |
| **string**   | 字符串，无需指定最大长度                             |               |
| timestamp    | 时间类型                                             |               |
| binary       | 二进制数据                                           |               |

集合数据类型如下：

| 类型   | 说明                           | 定义                          | 取值         |
| ------ | ------------------------------ | ----------------------------- | ------------ |
| array  | 数组是一组相同类型的值的集合   | `array<string>`               | `arr[0]`     |
| map    | map 是一组相同类型的键值对集合 | `map<string, int>`            | `map['key']` |
| struct | 结构体                         | `struct<id:int, name:string>` | `struct.id`  |

!!! note "通过集合类型来定义列的好处是什么?"

在大数据系统中，不遵循标准格式的一个好处就是可以**提供更高吞吐量**的数据。当处理的数据的数量级是 T 或者 P 时，以最少的「头部寻址」来从磁盘上扫描数据是非常必要的。**按数据集进行封装的话可以通过减少寻址次数来提供查询的速度**。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。

!!! note "集合类型的应用场景"

- map 类型常用来做扩展字段，避免多次修改表结构
- array 类型常常出现在行列转化中

Hive 的基本数据类型可以做**类型转换**，转换的方式包括隐式转换以及显式转换。

隐式转换：

- 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint
- 所有整数类型、float 和 **string** 类型都可以隐式地转换成 double
- tinyint、smallint、int 都可以转换为 float
- boolean 类型不可以转换为任何其它的类型

---

## Hive 读写文件流程

当进程在进行远程通信时，发送方需要把对象转化为字节序列才可在网络上传输，称为**对象序列化**；接收方则需要把字节序列恢复为对象， 称为对象的**反序列化**。

读过程：HDFS files --> InputFileFormat --> `<key,value>` --> Deserializer(反序列化) --> Row Object

Hive 读取文件时首先调用 InputFormat（默认 TextInputFormat），返回一条一条 kv 键值对记录（默认是一行对应一条记录）。然后调用 SerDe（默认 LazySimpleSerDe）的 Deserializer，将一条记录中的 value 根据分隔符切分为各个字段。

写过程：Row Object --> serializer(序列化) --> `<key,value>` --> OutputFileFormat --> HDFS files

Hive 写文件时，首先调用 SerDe（默认 LazySimpleSerDe）的 Serializer 将对象转换成字节序列，然后调用 OutputFormat 将数据写入 HDFS 文件中。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191430557.png)

在 Hive 的 HQL 语句中，select 时将会用到反序列化操作， insert 时会用到序列化操作。

---

## 内部表和外部表的区别

- 存储位置：内部表的数据由 Hive 自身管理，存储在数据仓库指向的路径（默认为 `user/hive/warehouse`）；外部表的数据存储在 HDFS 中，Hive 仅仅记录数据的所在路径
- 删除表之后：内部表删除之后，表的元数据和实际数据都被删除；外部表删除后，仅仅删除表的元数据，实际数据不会被删除
- 建表语法：外部表在创建时需要加上 external 关键字
- 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要修复（`MSCK REPAIR TABLE table_name;`）

应用场景：

- 外部表适合将已经存在的数据文件导入到 Hive 中进行只读的分析，也能方便共享来自其他系统的数据
- 内部表由于删除时会同时删除数据文件，对内部表的结构修改也会同步给元数据，因此适合对表进行修改、删除操作。如做 etl 处理时，通常会选择内部表做中间表，因为清理时，会将 HDFS 上的文件同时删除

> 如何查看一个表是内部表还是外部表？

```sql
describe extended tablename;
```

> 内部表和外部表的转换

```sql
--- 内部表转外部表
alter table testA set TBLPROPERTIES ("EXTERNAL"="TRUE")
--- 外部表转换内部表
alter table testB set TBLPROPTIES ("EXTERNAL"="FALSE")
```

---

## 分区表

**分区是将表按照某个列的值进行划分，然后将表的数据按照不同分区分散存储在不同的目录下**。

分区的目的主要是为了**提高查询性能**，尤其是对于大规模的表，使用分区可以提高查询性能，减少扫描的数据量。另外，分区还可以提高查询的并发度。

分区是以字段的形式在表结构中存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。

创建分区表：

```sql
create table dept_partition
(
	deptno int, --部门编号
	dname string, --部门名称
	loc string --部门位置
)
partitioned by (day string)
row format delimited fields terminated by '\t';
```

将数据写入分区表：

- load 方式

```sql
load data local inpath '/datas/dept_20220401.log'
into table dept_partition
partition(day='20220401');
```

- insert 方式

将 day='20220401' 分区的数据复制一份插入到 day='20220402' 分区

```sql
insert overwrite table dept_partition partition(day = '20220402')
select deptno, dname, loc
from dept_partition
where day = '2020-04-01';
```

修复分区：

Hive 将分区表的所有分区信息都保存在了元数据中，只有元数据与 HDFS 上的分区路径一致时，分区表才能正常读写数据。若用户手动创建/删除分区路径，Hive 都是感知不到的，这样就会导致 Hive 的元数据和 HDFS 的分区路径不一致。

若分区元数据和 HDFS 的分区路径不一致，还可使用 msck 命令进行修复

```sql
msck repair table table_name [add/drop/sync partitions];
```

### 静态分区和动态分区

动态分区是指向分区表 insert 数据时，被写入的分区不由用户指定，而是**由每行数据的最后一个字段的值来动态地决定。使用动态分区，可只用一个 insert 语句将数据写入多个分区**。

静态分区与动态分区的主要区别在于：**静态分区是手动指定，而动态分区是通过数据来进行判断**。详细来说，静态分区的列是在编译时期，通过用户传递来确定的；动态分区只有在 SQL 执行时才能确定。

静态分区：

```sql
insert into table test partition(dt='20210101')
select a, b, c
from source
where dt = '20210101';
--- test 字段：a, b, c, dt
```

动态分区：

```sql
insert into table test partition(dt)
select a, b, c, dt
from source
where dt >= '20210101';
--- 可以写入多个分区
```

要使用动态分区，需要开启动态分区功能，并且设置 Hive 为非严格模式。

---

## 分桶表

分桶是按照某个列的哈希值进行分桶，将表分散存储到不同的桶中，每个桶的数据量大致相同。

与分区的区别：分区表针对是目录，也就是存储路径，分桶表则针对的是文件，粒度更细，是另一种管理数据的方式。

数据分桶的原理: Hive 中按照分桶字段的 hash 值去模以分桶的个数。

**分桶的作用：**

- **提高查询效率**

全表查询转换成桶查询，查询的并发度更高。

- **大表 join**

对于两张大表的 join，执行 reduce join(shuffle join)肯定不合适，只能做 map join。但是 reduce join 只适合做小表和大表的 join，如何做大表间的 join 呢？此时可以**对两张大表的连接字段分桶**，此时的 join 是按照桶来的，字段值相同的 record 会进入同一个桶。

需要注意的是：两张大表的分桶数量上必须是倍数关系，确保对于连接条件的一致性。

解释：如果连接字段某一个值的 hashcode 为 8，A 表分 4 桶，B 表分 5 桶，则该 值在 A 表进入 0 号桶，在 B 表进入 3 号桶，此时 join 的连接条件就不一致了。 要想在 map 端执行桶 join，必须设置 `hive.optimize.bucketmapjoin=true`。

- **抽样查询**

分桶可以用于对表进行抽样，对于大规模的表，无需扫描全表即可对数据集进行抽样分析。

!!! note "分桶和分区的区别"

- 分区是对应不同的目录（粗粒度），分桶是对应不同的文件（细粒度）。
- 分区是非随机地分割数据，而分桶是按照列的哈希函数进行分割的，因此是随机分割的，也相对比较平均。
- 分区通常是对数据进行逻辑上的分类和组织，分桶则常常用于优化 join 操作或者对大规模数据进行抽样。

!!! question "分桶为什么可以优化 join 操作？"

分桶主要是用于优化大表和大表的 join，如果大表 join 大表采用 reduce join 的话，不仅慢，还可能 shuffle 数据量过大产生异常。

- 分桶可以将大表的数据划分为一个个桶，分别在 Map 端做 join。由于分桶时，需要计算分桶字段的哈希值 % 分桶个数的一个值，根据这个值将数据放到不同的桶里。这样可以保证相同 key 的数据会在一个桶里，因此在 join 的时候不需要全表扫描，只需要去扫描对应桶中的数据即可。
- 由于分桶表的每一个桶中的数据都是有序的，因此可以更高效地做 map join（SMB Map Join/Sort Merge Bucket Map Join）。

核心思想是将大表转换为小表，分别做 Map Join。

---

## Order By, Sort By, Distribute By, Cluster By

- Order By：会对查询结果集执行一个全局排序，所有数据都通过一个 reducer 进行处理
- Sort By：只能保证每个 reducer 内部的数据局部有序，在数据进入 reducer 前完成排序
- Distribute By：按照指定字段对数据进行划分，输出到不同的 reducer 中，通常是为了进行后续的聚集操作，常与 sort by 配合使用
- Cluster By：如果 distribute by 和 order by 的字段相同，并且升序排序，则可以使用 cluster by 简写

---

## Join 的 MR 实现原理

### Common Join/Reduce Join

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170306655.png)

如果不指定 Map Join 或者不符合 Map Join 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即在 Reduce 阶段完成 join。 整个过程包含 Map、Shuffle、Reduce 阶段。

**Map 阶段：**

- Step1：读取源表的数据，Map 输出时候以 Join on 条件中的字段为 key，如果 Join 有多个关联键，则以这些关联键的组合作为 key
- Step2: Map 输出的 value 为 join 之后所关心的(select 或者 where 中需要用到的) 列；同时在 value 中还会包含表的 Tag 信息，用于标明此 value 对应哪个表
- Step3: 按照 key 进行排序。

**Shuffle 阶段：**

- 根据 key 的值进行 hash，并将 key/value 按照 hash 值分配给不同的 reducer，这样能确保两个表中相同的 key 位于同一个 reducer 中。

**Reduce 阶段：**

- 根据 key 的值完成 join 操作，期间通过 Tag 来识别不同表中的数据。

---

### Map Join

Map Join 通常用于小表 join 大表的场景，具体小表有多小，由参数 `hive.mapjoin.smalltable.filesize` 来决定，该参数表示小表的总大小。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191954088.png)

Map Join 可以通过两个只有 map 阶段的 Job 完成一个 join 操作。若某 join 操作满足要求，则第一个 Job 会读取小表数据，将其制作为哈希表，并上传至 **Hadoop 分布式缓存** (本质上是上传至 HDFS)。第二个 Job 会先从分布式缓存中读取小表数据，并缓存在 MapTask 的内存中，然后扫描大表数据，这样在 Map 端即可完成关联操作。

- Job1（本地任务）：读取小表数据、制作哈希表、上传至分布式缓存

- Job2：从分布式缓存读取哈希表、加载至内存、完成 join

**Map Join 的优点是只有 Map 阶段，不存在 Shuffle 和 Reduce 阶段，因此效率非常高**。

触发 Map Join：

- 在 SQL 语句中增加 hint 提示（过时，不推荐使用）
- Hive 优化器根据表的数据量大小自动触发

自动触发：

Hive 在编译 SQL 语句阶段，起初所有的 Join 操作均采用 Common Join 算法实现。

之后在物理优化阶段，Hive 会根据每个 Common Join 任务所需表的大小判断该 Common Join 任务是否能够转换为 Map Join 任务，若满足要求，便将 Common Join 任务自动转换为 Map Join 任务。判断标准为小表大小与用户设置的阈值（MapTask 内存）比较。

但**有些 Common Join 任务所需的表大小，在 SQL 的编译阶段是未知的**（例如先进行一个分组聚合，之后再和一张表进行 Join 操作），所以这种 Common Join 任务是否能转换成 Map Join 任务在编译阶是无法确定的。

针对这种情况，Hive 会在编译阶段生成一个**条件任务 (Conditional Task)**，其下会包含一个**计划列表**，计划列表中包含**转换后的 Map Join 任务以及原有的 Common Join 任务**。 最终具体采用哪个计划，是在**运行时决定**的。大致思路如下图所示:

假设我们有 A 和 B 两张表，两张表的大小都未知，那么此时 Map Join 有 2 种情况：

- 缓存 A 表，扫描 B 表
- 缓存 B 表，扫描 A 表

因此会生成多种执行计划。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160539533.png)

---

### Bucket Map Join

Bucket Map Join 是**对 Map Join 的改进**，其打破了 Map Join 只适用于大表 join 小表的限制，可用于大表 join 大表的场景。

**分桶的原理：对分桶字段进行 hash partition，拆分为若干个文件。**

Bucket Map Join 的核心思想是：若能保证参与 join 的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与 join 的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行 Map Join 操作了。这样一来，第二个 Job 的 Map 端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160450333.png)

对于上面的图：

对于 Table B 来说，由于有 2 个桶，因此所有 `hashcode % 2 == 0` 的 record 都会进入 Bucket B-0，所有 `hashcode % 2 == 1` 的 record 都会进入 Bucket B-1。

对于 Table A 来说，由于有 4 个桶，因此所有 `hashcode % 4 == 0` 的 record 都会进入 Bucket A-0。所有 `hashcode % 4 == 2` 的 record 都会进入 Bucket A-2。Bucket A-0 和 Bucket A-2 中的这些 record 就对应着 Bucket B-0 中的 record。同理，Bucket A-1 和 Bucket A-3 中的所有 record 就对应着 Bucket B-1 中的 record。

当然，最简单的情况就是两张表的桶数相同。

之后就和 Map Join 的操作一致：

- Job1：先由本地 Map 任务将相对小一点的表的每个桶各制作一张哈希表，将所有哈希表上传至分布式缓存
- Job2：按照 BucketInputFormat 读取大表数据，一个桶一个切片，每一个 Mapper 只需要处理大表中的一个桶即可。同时，根据桶之间的对应关系，每个 Mapper 只需要从分布式缓存中读取自己需要的小表的桶的哈希表即可。例如，Mapper1 被分到了 Bucket A-0，因此，Mapper1 只需要去分布式缓存读取 Bucket B-0 即可。

!!! note "总结"

    **Bucket Map Join 适用于大表 Join 大表的情况，实现方式是先对 2 张大表进行分桶，之后对每个桶进行 Map Join**。具体地，每个 Mapper 只需要处理一个桶的数据，并且只需要从分布式缓存中读取自己需要 join的桶的哈希表即可。

---

### SMB Map Join

Sort Merge Bucket Map Join(简称 SMB Map Join) 基于 Bucket Map Join。SMB Map Join 要求，参与 join 的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、 排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。

SMB Map Join 同 Bucket Join 一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行 join 操作，==不同的是分桶之间的 join 操作的实现原理。Bucket Map Join，两个分桶之间的 join 实现原理为 **Hash Join 算法**；而 SMB Map Join，两个分桶之间的 join 实现原理为 **Sort Merge Join 算法**==。

Hash Join 和 Sort Merge Join 均为关系型数据库中常见的 Join 实现算法。Hash Join 的原理相对简单，就是对参与 join 的一张表构建哈希表，然后扫描另外一张表，然后进行逐行匹配。**Sort Merge Join 需要在两张按照关联字段排好序的表中进行**，其原理如图所示：

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310160518467.png)

**SMB Map Join VS Bucket Map Join**

- 不需要制作哈希表
- 不需要在内存中缓存哈希表（一个桶），因此对桶的大小没有要求

Hive 中的 SMB Map Join 就是对两个分桶的数据按照上述思路进行 Join 操作。可以看出，SMB Map Join 与 Bucket Map Join 相比，在进行 Join 操作时，Map 端是无需对整个 Bucket 构建哈希表，也无需在 Map 端缓存整个 Bucket 数据的，每个 Mapper 只需按顺序逐个 key 读取两个分桶的数据进行 join 即可。

设置：

```sql
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
```

---

## 自定义函数

Hive 自定义函数包括三种 UDF、UDAF、UDTF。

- UDF(User-Defined-Function)：用户自定义函数，一对一的输入输出。
- UDAF(User-Defined Aggregation Funcation)：用户自定义聚合函数，多进一出。 例如：count/max/min。
- UDTF(User-Defined Table-Generating Functions)：用户自定义表生成函数，一对多的输入输出。例如：lateral view explode。

使用方式：

在 HIVE 会话中 add 自定义函数的 jar 文件，然后创建 function 继而使用函数。具体步骤如下：

1. 编写自定义函数
2. 打包上传到集群机器中
3. 进入 hive 客户端，添加 jar 包：`hive> add jar /home/hive_udf.jar`
4. 创建临时函数： `hive> create temporary function getLen as 'com.anson.GetLength';`
5. 使用临时函数： `hive> select getLen('1234567');`
6. 销毁临时函数：`hive> drop temporary function getLen;`

---

### UDF

一对一的输入输出，只能输入一条记录当中的数据，同时返回一条处理结果。属于最常见的自定义函数。Hive 的 UDF 有两种实现方式或者实现的 API，一种是 UDF 比较简单，一种是 GenericUDF 比较复杂。

如果所操作的数据类型都是基础数据类型，如（Hadoop&Hive 基本 writable 类型，如 Text, IntWritable, LongWriable, DoubleWritable 等）。那么继承 `org.apache.hadoop.hive.ql.UDF` 类，并实现 evaluate() 方法即可。

如果所操作的数据类型是复杂数据类型，如 Map，List 和 Set，那么需要继承 `org.apache.hadoop.hive.ql.udf.generic.GenericUDF` 类，并实现三个方法：

1. initialize()：只调用一次，在任何 evaluate() 调用之前可以接收到一个可以表示函数输入参数类型的 ObjectInspectors 数组。initialize 用来**验证该函数是否接收正确的参数类型和参数个数**，最后提供最后结果对应的数据类型。
2. evaluate()：真正的逻辑，读取输入数据，处理数据，返回结果。
3. getDisplayString()：返回描述该方法的字符串，没有太多作用。

todo：实现一个手机号脱敏的 UDF

---

### UDAF

[Hive 自定义函数(UDF、UDAF)\_51CTO 博客\_hive 自定义函数](https://blog.51cto.com/liguodong/2992519)

实现方式：

1. 自定义一个 Java 类，继承 UDAF 类
2. 内部定义一个静态类，实现 UDAFEvaluator 接口
3. 实现 init(), iterate(), terminatePartial(), merge(), terminate()，共 5 个方法

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310200232537.png)

---

### UDTF

需要继承 `org.apache.hadoop.hive.ql.udf.generic.GenericUDTF` 类，并实现三个方法：

1. initialize()：返回 UDTF 的返回行的信息（返回个数，类型）。
2. process()：真正的处理过程在 process 函数中，在 process 中，每一次 forward() 调用产生一行；如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到 forward()函数。forward()传入的就是最后的结果，里面一般是数组，数组有多少个元素就代码最后一行输出的结果有多少列
3. close()：对需要清理的方法进行清理。

---

## Hive 数据存储格式

### 行存储和列存储

行存储特点：

- 需要查询一整行的数据的时候，行存储由于一行数据都聚集在一块，因此这种情况下查询更快。而如果仅仅查询少数几列数据时，则查询效率不高。
- 一行数据通常包含不同的数据类型，因此难以获得理想的压缩效果。

列存储特点：

- **更少的 I/O**：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量。
- **更高的压缩比**：每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。

Hive 中常用的存储格式有 TEXTFILE 、SEQUENCEFILE、AVRO、RCFILE、ORCFILE、 PARQUET 等，其中 TEXTFILE 、SEQUENCEFILE 和 AVRO 是行式存储，RCFILE、ORCFILE、PARQUET 是列式存储。

---

### Hive 中常见的存储格式

Apache Hive 支持 Apache Hadoop 中使用的几种熟悉的文件格式，如 TextFile，RCFile，SequenceFile，AVRO，ORC 和 Parquet 格式。

**TextFile** 每一行都是一条记录，每行都以换行符结尾。**数据不做压缩，磁盘开销大，数据解析开销大**。可结合 Gzip、Bzip2 使用（系统自动检查，执行查询时自动解压），但**使用这种方式，Hive 不会对数据进行切分，从而无法对数据进行并行操作**。

**SequenceFile** 是一种**二进制文件**，它将数据**以 `<key,value>` 的形式序列化到文件中**。这种二进制文件内部使用 Hadoop 标准的 Writable 接口实现序列化和反序列化。**支持不同级别的压缩**：NONE, RECORD, BLOCK。

**AVRO** 是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro 能够提供更好的**序列化和反序列化性能**。

**RCFile** 是一种**行列存储相结合**的存储方式。将文件水平划分为多个**行组（Row Group）**，行组内部的数据是按列存储的。因此它能保证同一行的数据位于同一个 HDFS 块，同时还能利用列存储的优点进行快速的列读取和数据压缩。

**ORC** 对 RCFile 做了一些优化，主要包括支持更复杂的数据类型（datetime, decimal, struct, list 等）、在文件中存储了一些轻量级的稀疏索引等。ORC 也是一种行列存储相结合的存储方式。**它将文件水平划分为多个 Stripe，每个 Stripe 内部的数据是按列存储的**。它和 RCFile 一样，能保证同一行的数据位于同一个 HDFS 块，同时还能利用列存储的优点进行快速的列读取和数据压缩。此外，它还能利用稀疏行索引，在 stripe 中快速读取的过程中可以跳过很多行。

**Parquet** 是一个面向列的二进制文件格式。**它支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能**。Parquet 将文件水平划分为若干个 Row Group，每个 Row Group 包含多个 Column Chunk，每个 Column Chunk 包含多个 Page。

---

### RC File

RCFile（Record Columnar File）存储结构遵循的是「先水平划分，再垂直划分」 的设计理念。RCFile 结合了行存储和列存储的优点：首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231818368.png)

如上图是 HDFS 内 RCFile 的存储结构。每个 HDFS 块中，RCFile 以**行组**为基本单位来组织记录。对于一张表，所有行组大小都相同。一个 HDFS 块会有一个或多个行组。一个行组包括三个部分：

- 第一部分是行组头部的同步标识，主要用于分隔 HDFS 块中的两个连续行组；
- 第二部分是行组的元数据头部，用于存储行组单元的信息，包括行组中的记录数、每个列的字节数、列中每个域的字节数；
- 第三部分是表格数据段，即实际的列存储数据。

相比于其他列式存储的优势：

- 某些列式存储同一列可能存在不同的 block 上，在查询的时候，Hive 重组列的过程会浪费很多 IO 开销。而 RCFile 由于相同的列都是在一个 HDFS 块上，所以 相对列存储而言会节省很多资源。
- RCFile 采用**游程编码**，相同的数据不会重复存储，很大程度上节约了存储空间，尤其是字段中包含大量重复数据的时候。

注：游程编码是一种简单的压缩算法，如字符串「AAABBBCCC」可以用「3A3B3C」来表示。对于重复并且连续出现的数据压缩较为有效。

- RCFile 不支持任意方式的数据写操作，仅提供一种追加接口，这是因为底层的 HDFS 当前仅仅支持数据追加写文件尾部。
- 当处理一个行组时，RCFile 无需全部读取行组的全部内容到内存。相反，它仅仅读元数据头部和给定查询需要的列。因此，它**可以跳过不必要的列以获得列存储的 I/O 优势**。例如：`select c from table where a > 1`。

---

### ORC File

ORC File，它的全名是 Optimized Row Columnar (ORC) file，其实就是对 RCFile 做了一些优化。运用 ORC File 可以提高 Hive 的读、写以及处理数据的性能。

和 RCFile 格式相比，ORC File 格式有以下优点：

- 每个 task 只输出单个文件，这样可以减少 NameNode 的负载
- 支持各种复杂的数据类型，比如： datetime, decimal, 以及一些复杂类型 (struct, list, map, and union)
- 在文件中存储了一些轻量级的索引数据
- 基于数据类型的块模式压缩：integer 类型的列用游程编码 (run-length encoding)；String 类型的列用字典编码

ORC File 包含一组组的行数据，称为 stripe，除此之外，ORC File 的 file footer 还包含一些额外的辅助信息。在 ORC File 文件的最后，有一个被称为 postscript 的区，它主要是用来存储压缩参数及压缩页脚的大小。 在默认情况下，一个 stripe 的大小为 250MB。大尺寸的 stripe 使得从 HDFS 读数据更高效。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231834503.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231901999.png)

Index data 包含每列的最大和最小值以及每列所在的行。行索引里面提供了偏移量，它可以跳到正确的压缩块位置。具有相对频繁的行索引，使得在 stripe 中快速读取的过程中可以跳过很多行，尽管这个 stripe 的大小很大。在默认情况下，最大可以跳过 10000 行。拥有通过过滤谓词而跳过大量的行的能力。

- index data：每隔一万行的稀疏索引
- row data：实际数据
- stripe footer：数据的长度，类型等信息
- file footer：记录了各个 stripe 的信息，每个 stripe 中有多少行，以及每列的数据类型。当然，它里面还包含了列级别的一些聚合的结果， 比如：count, min, max, and sum
- postscript：记录了 file footer 的长度

使用 ORC File：

```sql
CREATE TABLE ... STORED AS ORC
ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
SET hive.default.fileformat=Orc
```

```sql
create table Addresses ( name string, street string, city string, state string, zip int )
stored as orc tblproperties ("orc.compress"="NONE")
```

---

### Parquet

关系数据库中使用数据模型通常都是扁平式的，但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能**需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的**，所以**如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果**。

Parquet 的灵感来自于 2010 年 Google 发表的 Dremel 论文，文中介绍了一种**支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能**。Parquet 仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是 parquet 相较于 ORC 的仅有优势：支持嵌套结构。Parquet 没有太多其他可圈可点的地方，比如他不支持 update 操作(数据写成后不可修改)，不支持 ACID 等。

Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310231900429.png)

上图展示了一个 Parquet 文件的基本结构，文件的首尾都是该文件的 Magic Code，用于校验它是否是一个 Parquet 文件。

首尾中间由若干个 Row Group 和一个 Footer(File Meta Data) 组成。

每个 Row Group 包含多个 Column Chunk，每个 Column Chunk 包含多个 Page。以下是 Row Group、Column Chunk 和 Page 三个概念的说明:

- 行组 (Row Group)： 一个行组对应逻辑表中的若干行。
- 列块 (Column Chunk)： 一个行组中的一列保存在一个列块中。
- 页 (Page)：一个列块的数据会划分为若干个页。

Footer(File Meta Data) 中存储了每个行组 (Row Group) 中的每个列块 (Column Chunk) 的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的 Data Page 位置等信息。

除了支持嵌套结构，Parquet 还支持一些优化：

- 映射下推(Project PushDown)

说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现 TableScan 算子，而避免扫描整个表文件内容。

在 Parquet 中原生就支持映射下推，执行查询的时候可以通过 Configuration 传递需要读取的列的信息，映射每次会扫描一个 Row Group 的数据，然后一次性得将该 Row Group 里所有需要的列的 Column Chunk 都读取到内存中，每次读取一个 Row Group 的数据能够大大降低随机读的次数， 除此之外，Parquet 在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。

- 谓词下推(Predicate PushDown)

通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能。Parquet 做了更进一步的优化，优化的方法是对每一个 Row Group 的每一个 Column Chunk 在存储的时候都**计算对应的统计信息**，包括该 Column Chunk 的最大值、最小值和空值个数。**通过这些统计值和该列的过滤条件可以判断该 Row Group 是否需要扫描**。另外 Parquet 未来还会增加诸如 Bloom Filter 和 Index 等优化数据，更加有效的完成谓词下推。

在使用 Parquet 的时候可以通过如下两种策略提升查询性能：

- 类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。
- 减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的 I/O 负载。

**性能对比：**

相比传统的行式存储，Hadoop 生态圈近年来也涌现出诸如 RC、ORC、Parquet 的列式存储格式，它们的性能优势主要体现在两个方面：

1. 更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。
2. 更少的 I/O，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能。

在数据存储方面，ORC 和 Parquet 两种存储格式在都是用 snappy 压缩的情况 下两种存储格式占用的空间相差并不大，而且 Parquet 格式稍好于 ORC 格式。两者在功能上也都有优缺点，Parquet 原生支持嵌套式数据结构，而 ORC 对此支持较差，这种复杂的 Schema 查询也相对较差；而 Parquet 不支持数据的修改和 ACID，但是 ORC 对此提供支持，但是在 OLAP 环境下很少会对单条数据修改，更多的则是批量导入。

---

## Hive 中的数据压缩

数据压缩优点：减少存储磁盘空间，降低单节点的磁盘 I/O。减少网络传输的开销。

数据压缩缺点：需要花费额外的时间/CPU 做压缩和解压缩计算。

具体使不使用数据压缩，具体取决于 job 类型：对那些 I/O 密集型的作业应该使用数据压缩，而对于 CPU 密集型的作业，使用压缩反而会降低性能。

数据压缩可用于：

- 表数据
- Map 阶段输出
- Reduce 阶段输出

对于一个压缩算法，我们通常从**压缩比**、**压缩解压缩速度**、**压缩后文件是否可以分割**来进行评价。

| 压缩方式 | 压缩比 | 压缩速度 | 解压缩速度 | 是否可分割 |
| -------- | ------ | -------- | ---------- | ---------- |
| gzip     | 13.4%  | 21 MB/s  | 118 MB/s   | 否         |
| bzip2    | 13.2%  | 2.4MB/s  | 9.5MB/s    | 是         |
| lzo      | 20.5%  | 135 MB/s | 410 MB/s   | 是         |
| snappy   | 22.2%  | 172 MB/s | 409 MB/s   | 否         |

GZip 和 BZip2 压缩可以保证最小的压缩文件，但是过于消耗时间；Snappy 和 LZO 压缩和解压缩很快，但是压缩的文件较大。

---

## Hive 调优

对于 Hive 中的调优问题，应该优先考虑 **Join**、**分组聚合**和**数据倾斜**的优化。其次是从 Hadoop/MR 的角度。然后是 Hive 的一些设置。

### Join 的优化

对于两表之间的 Join，我们应该优先考虑是否可以在 Map 端执行 Join，避免 Shuffle 和 Reduce 阶段，从而减少 I/O 和网络传输的开销。

- 对于小表 Join 大表的情况，Hive 会自动帮我们转换为 Map Join
- 对于大表 Join 大表的情况，如果两张表都是分桶表，并且连接字段是分桶字段，则可以使用 Bucket Map Join，Join 算法为 Hash Join
- 对于大表 Join 大表的情况，如果两张表都是分桶表，并且连接字段是分桶字段，并且还是有序的，则可以使用 SMB Join，Join 算法为 Sort Merge Join

### 分组聚合的优化

**Group By 的优化：**

分组聚合和 Join 同理，也是优先考虑是否可以在 Map 端执行聚合，从而减少 Shuffle 阶段的数据量。

Hive 中未经优化的分组聚合，是通过一个 MapReduce Job 实现的。Map 端负责读取数据，并按照分组字段分区，通过 Shuffle，将数据发往 Reduce 端，各组数据在 Reduce 端完成最终的聚合运算。

Hive 对分组聚合的优化主要围绕着**减少 Shuffle 数据量**进行，具体做法是 **map-side 聚合**。所谓 map-side 聚合，就是在 map 端维护一个（内存中的）哈希表，利用其完成部分的聚合，然后将部分聚合的结果，按照分组字段分区，发送至 reduce 端，完成最终的聚合。**map-side 聚合能有效减少 shuffle 的数据量，提高分组聚合运算的效率**。

**COUNT(DISTINCT) 的优化：**

数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换。

```sql
--直接去重
select count(distinct id) from bigtable;
--改写后去重
select count(id) from (select id from bigtable group by id) a;
```

虽然会多用一个 Job 来完成，但在数据量大的情况下，group by 依旧是去重的一个优化手段。如果说需要统计的字段有 Null 值，最后只需要 null 值单独处理后 union 即可。

---

### 数据倾斜的优化

数据倾斜问题，通常是指参与计算的数据分布不均，即某个 key 或者某些 key 的数据量远超其他 key，导致在 shuffle 阶段，大量相同 key 的数据被发往同一个 Reducer，进而导致该 Reducer 所需的时间远超其他 Reducer，成为整个任务的瓶颈。

Hive 中的数据倾斜常出现在**分组聚合**和 **Join** 操作的场景中。

#### Group By 产生数据倾斜

使用 Hive 对数据做分组聚合的时候某种类型的数据量特别多，而其他类型数据的数据量特别少。

Hive 中未经优化的分组聚合，是通过一个 MapReduce Job 实现的。Map 端负责读取数据，并按照分组字段分区，通过 Shuffle，将数据发往 Reduce 端，各组数据在 Reduce 端完成最终的聚合运算。

如果 group by 分组字段的值分布不均，就可能导致大量相同的 key 进入同一 Reducer， 从而导致数据倾斜问题。

由分组聚合导致的数据倾斜问题，有以下两种解决思路:

- **Map-Side 聚合**

开启 Map-Side 聚合后，数据会现在 Map 端完成部分聚合工作。这样经过 Map 端的初步聚合后，可以缓解发往 Reducer 的数据的倾斜程度。最佳状态下，Map 端聚合能完全屏蔽数据倾斜问题。

- **Skew-GroupBy 优化**

Skew-GroupBy 的原理是启动两个 MR 任务，第一个 MR 按照随机数分区，将数据分散发送到 Reducer，完成部分聚合，第二个 MR 按照分组字段分区，完成最终聚合。

```sql
--启用分组聚合数据倾斜优化
set hive.groupby.skewindata=true;
```

- 根据业务，合理调整分组维度

可以单独将倾斜程度大的字段单拎出来计算，再 union 结果。

---

#### count(distinct) 产生数据倾斜

如果数据量非常大，执行如 `select a, count(distinct b) from t group by a;` 类型的 SQL 时，会出现数据倾斜的问题。

解决方法：

- 使用 sum ... group by 代替。

```sql
select a, sum(1) from (select a,b from t group by a,b) group by a;
```

详见上文 count(distinct) 优化

- 在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理，最后 union 回去。

---

#### Join 产生数据倾斜

未经优化的 Join 操作，默认是使用 common join 算法，也就是通过一个 MapReduce Job 完成计算。Map 端负责读取 join 操作所需表的数据，并按照关联字段进行分区，通过 Shuffle，将其发送到 Reduce 端，相同 key 的数据在 Reduce 端完成最终的 Join 操作。

如果关联字段的值分布不均，就可能导致大量相同的 key 进入同一 Reducer，从而导致数据倾斜问题。

由 Join 导致的数据倾斜问题，有如下三种解决方案：

- **Map Join**

使用 map join 算法，join 操作仅在 map 端就能完成，没有 shuffle 操作，没有 reduce 阶段，自然不会产生 reduce 端的数据倾斜。该方案适用于**大表 join 小表时发生数据倾斜的场景**。

- **Skew Join**

Skew Join 的原理是，为倾斜的大 key 单独启动一个 map join 任务进行计算，其余 key 进行正常的 common join。原理图如下：

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310161600880.png)

注意：在数据倾斜时，如果是**大表 Join 大表**，那么也无法使用 Bucket Map Join，因为分桶（过程为 MR）也是倾斜的。此时需要使用 Skew Join。

```sql
--启用 skew join 优化
set hive.optimize.skewjoin=true;
--触发 skew join 的阈值，若某个 key 的行数超过该参数值，则触发
set hive.skewjoin.key=100000;
```

这种方案对参与 join 的源表大小没有要求，但是对两表中倾斜的 key 的数据量有要求， 要求一张表中的倾斜 key 的数据量比较小 (方便走 map join)。

- **调整 SQL 语句**

若参与 join 的两表均为大表，其中一张表的数据是倾斜的，此时也可通过以下方式对 SQL 语句进行相应的调整。

假设原始 SQL 语句如下：A，B 两表均为大表，且其中一张表的数据是倾斜的。

```sql
select *
from A join B
on A.id=B.id;
```

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310161615260.png)

图中 1001 为倾斜的大 key，可以看到，其被发往了同一个 Reduce 进行处理。

调整 SQL 语句如下：

```sql
select
	*
from(
    select --打散操作
    	concat(id,'_',cast(rand()*2 as int)) id, value
	from A
)ta join(
	select --扩容操作
        concat(id,'_',0) id, value
    from B
    union all
    select
        concat(id,'_',1) id, value
    from B
)tb
on ta.id=tb.id;
```

调整之后的 SQL 语句执行计划如下图所示：

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310161619269.png)

---

#### 空值产生数据倾斜

遇到需要进行 join 的但是关联字段有数据为空。如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和用户表中 的 user_id 关联，会碰到数据倾斜的问题。数据量大时也会产生数据倾斜，如表一的 id 需要和表二的 id 进行关联。

解决方法：

- 单独把空值拎出来再 union

```sql
select a.* from log a join users b
on a.user_id is not null and a.user_id = b.user_id
union all
select a.* from log a where a.user_id is null;
```

- 给空值分配随机的 key 值

```sql
select * from log a left outer join users b
on case when a.user_id is null then
concat('hive', rand()) else a.user_id end = b.user_id;
```

一般分配随机 key 值得方法更好一些。

---

### 其他优化

#### 分区裁剪、列裁剪

尽可能早地过滤掉不需要的数据。

#### Fetch 抓取

Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：`select * from emp;` 在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出查询结果到控制台。

```sql
--是否在特定场景转换为 fetch 任务
--设置为 none 表示不转换
--设置为 minimal 表示支持 select *，分区字段过滤，Limit 等
--设置为 more 表示支持 select 任意字段, 包括函数，过滤，和limit等
set hive.fetch.task.conversion=more;
```

#### 本地模式

有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。**对于小数据集，执行时间可以明显被缩短**。

```sql
--开启自动转换为本地模式
set hive.exec.mode.local.auto=true;

--设置 local MapReduce 的最大输入数据量，当输入数据量小于这个值时采用 local MapReduce的方式，默认为134217728，即128M
set hive.exec.mode.local.auto.inputbytes.max=50000000;

--设置 local MapReduce 的最大输入文件个数，当输入文件个数小于这个值时采用 local MapReduce 的方式，默认为 4
set hive.exec.mode.local.auto.input.files.max=10;
```

#### 严格模式

严格模式可以防止某些危险操作：

- 分区表不使用分区过滤
- 使用 order by 没有 limit 过滤
- 笛卡尔积

#### 并行执行

Hive 会将一个 SQL 语句转化成一个或者多个 Stage，每个 Stage 对应一个 MR Job。默认情况下，Hive 同时只会执行一个 Stage。但是某 SQL 语句可能会包含多个 Stage，但这**多个 Stage 可能并非完全互相依赖，也就是说有些 Stage 是可以并行执行的**。此处提到的并行执行就是指这些 Stage 的并行执行。相关参数如下：

```sql
--启用并行执行优化，默认是 false
set hive.exec.parallel=true;

--同一个 sql 允许最大并行度，默认为 8
set hive.exec.parallel.thread.number=8;
```
