## Hive 概述

Hive 是由 Facebook 开源，基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。

Hive 本质上是一个 Hadoop 客户端，用于将 HQL 转化成 MapReduce 程序。

Hive 最适合于数据仓库应用程序，使用该应用进行相关的静态数据分析，不需要快速给出结果，而且数据本身不会频繁变化。

Hive 本身不是一个完整的数据库，Hadoop 和 HDFS 的设计本身约束和局限性限制了 Hive 所能胜任的工作。其中最大的限制就是 Hive 不支持记录级别的更新、插入或者删除操作，只能通过查询生产新表或者将查询结果导入到文件中。同时，Hive 也不支持事务。

---

## Hive 和关系型数据库的区别

OLTP VS OLAP

---

## Hive 的组成

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310290348457.png)

客户端组件：

- CLI：通过命令行的方式运行 Hive 客户端
- Web UI：通过网页访问 Hive 提供的服务

服务端组件：

- Driver：包含解析器、编译器、优化器和执行器，作用是对 HQL 进行解析、编译、优化、生成执行计划，提交给底层的执行引擎
- MetaStore：存储元数据（表名、表所属数据库、表类型、表数据所在目录等），默认使用自带的 Derby 数据库，但缺点是无法并发运行 2 个 Hive CLI 实例，因此推荐使用 MySQL 数据库来存储元数据
- HiveServer2：服务端接口，通过 Thrift 协议提供对 Hive 的远程访问，我们可以利用 JDBC/ODBC 来访问 Hive，支持多客户端并发和身份验证

---

## Hive SQL 编译过程

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310170140969.png)

Hive 将 SQL 转化为 MapReduce 任务，整个编译过程分为六个阶段:

1. 解析器（SQLParser）：**词法、语法解析**，将 SQL 字符串转换成抽象语法树（AST）
2. **语义解析**（Semantic Analyzer）：将 AST 进一步划分为一个个查询单元（QueryBlock），并将 Hive 中的元信息赋给每个 QueryBlock
3. 逻辑计划生成器（Logical Plan Gen）：通过语法树**生成逻辑执行计划**（OperatorTree）
4. 逻辑优化器（Logical Optimizer）：**对逻辑计划进行优化**（如谓词下推、投影剪切）
5. 物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划**生成物理计划**（遍历 OperatorTree，翻译为 MR 任务）
6. 物理优化器（Physical Optimizer）：**对物理计划进行优化**（对 MR 任务的变换，如 Map Join）

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110258864.png)

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310110303920.png)

---

## Hive 工作原理/执行流程

1. 用户提交查询任务给 Driver
2. Driver 将查询发送到编译器以生成执行计划
3. 编译器从 MetaStore 中获取所需要的元数据信息
4. 编译器对 HQL 进行编译，会经历语法解析（AST）、语义解析（QueryBlock）、生成逻辑计划、优化逻辑计划、生成物理计划、优化物理计划等过程
5. 编译器将最终的执行计划返回给 Driver
6. Driver 将执行计划提交给执行引擎（Execution Engine）去执行， 执行引擎将 MR 任务提交给 Job Tracker
7. 任务执行完后将结果返回给 Driver 并返回给用户

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310290348373.png)

---

## 为什么 MapReduce 的执行效率很低？

MapReduce 的执行效率很低，这种低效是由它的执行模式决定的，所有的 MapTask、ReduceTask 全部是以进程的方式执行的，要启动进程、销毁进程，即使可以开启 JVM 重用，但是也是用的时候开启，结束之后关闭，而且 JVM 成本很高。

随着时代的发展，人们开发出计算处理能力更强大的数据处理工具，如 Spark、 Tez 等，所以 Hive 底层所支持的执行引擎有 MapReduce、Spark、Tez。

[开源 SQL-on-Hadoop 系统一览 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/56782207)

---

## MetaStore 内嵌模式、本地模式、远程模式

- Embedded metastore：内嵌模式

其特点是：**hive 服务和 metastore 服务运行在同一个进程中，derby 服务也运行在该进程中**。该模式无需特殊配置。

- Local metastore：本地模式

其特点是：**hive 服务和 metastore 服务运行在同一个进程中，mysql 是单独的进程**，可以在同一台机器上，也可以在远程机器上。该模式只需将 `hive-site.xml` 中的 ConnectionURL 指向 mysql，并配置好驱动名、数据库连接账号即可。

- Remote metastore：远程模式

其特点是：**hive 服务和 metastore 在不同的进程内，可能是不同的机器**。 该模式需要将 `hive.metastore.local` 设置为 false，并将 `hive.metastore.uris` 设置为 metastore 服务器 URI，如有多个 metastore 服务器，URI 之间用逗号分隔。metastore 服务器 URI 的格式为 thrift://host:port。

---

## Hive 元数据数据库包含的具体内容

- 是什么：本质上是用来存储 hive 中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。

- 做什么：主要用作数据管理，包括查看数据表之间的血缘关系、查看数据存储、数据表的访问权限控制等。

常见表：

- DBS：该表存储 Hive 中所有数据库的基本信息
- DATABASE_PARAMS：该表存储数据库的相关参数（创建数据库时的 WITH DBPROPERTIES 中的内容就存在这）
- TBLS：该表中存储 Hive 表、视图、索引表的基本信息
- SERDES：该表存储序列化使用的类信息

---

## Hive 数据类型

| 基本数据类型 | 说明                                                 | 定义          |
| ------------ | ---------------------------------------------------- | ------------- |
| tinyint      | 1byte 有符号整数                                     |               |
| smallint     | 2byte 有符号整数                                     |               |
| **int**      | 4byte 有符号整数                                     |               |
| **bigint**   | 8byte 有符号整数                                     |               |
| boolean      | 布尔类型，true 或者 false                            |               |
| float        | 单精度浮点数                                         |               |
| **double**   | 双精度浮点数                                         |               |
| **decimal**  | 十进制精准数字类型                                   | decimal(16,2) |
| **varchar**  | 字符序列，需指定最大长度，最大长度的范围是 [1,65535] | varchar(32)   |
| **string**   | 字符串，无需指定最大长度                             |               |
| timestamp    | 时间类型                                             |               |
| binary       | 二进制数据                                           |               |

集合数据类型如下：

| 类型   | 说明                           | 定义                          | 取值         |
| ------ | ------------------------------ | ----------------------------- | ------------ |
| array  | 数组是一组相同类型的值的集合   | `array<string>`               | `arr[0]`     |
| map    | map 是一组相同类型的键值对集合 | `map<string, int>`            | `map['key']` |
| struct | 结构体                         | `struct<id:int, name:string>` | `struct.id`  |

!!! note "通过集合类型来定义列的好处是什么?"

在大数据系统中，不遵循标准格式的一个好处就是可以**提供更高吞吐量**的数据。当处理的数据的数量级是 T 或者 P 时，以最少的「头部寻址」来从磁盘上扫描数据是非常必要的。**按数据集进行封装的话可以通过减少寻址次数来提供查询的速度**。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。

!!! note "集合类型的应用场景"

- map 类型常用来做扩展字段，避免多次修改表结构
- array 类型常常出现在行列转化中

Hive 的基本数据类型可以做**类型转换**，转换的方式包括隐式转换以及显式转换。

隐式转换：

- 任何整数类型都可以隐式地转换为一个范围更广的类型，如 tinyint 可以转换成 int，int 可以转换成 bigint
- 所有整数类型、float 和 **string** 类型都可以隐式地转换成 double
- tinyint、smallint、int 都可以转换为 float
- boolean 类型不可以转换为任何其它的类型

---

## Hive 读写文件流程

当进程在进行远程通信时，发送方需要把对象转化为字节序列才可在网络上传输，称为**对象序列化**；接收方则需要把字节序列恢复为对象， 称为对象的**反序列化**。

读过程：HDFS files --> InputFileFormat --> `<key,value>` --> Deserializer(反序列化) --> Row Object

Hive 读取文件时首先调用 InputFormat（默认 TextInputFormat），返回一条一条 kv 键值对记录（默认是一行对应一条记录）。然后调用 SerDe（默认 LazySimpleSerDe）的 Deserializer，将一条记录中的 value 根据分隔符切分为各个字段。

写过程：Row Object --> serializer(序列化) --> `<key,value>` --> OutputFileFormat --> HDFS files

Hive 写文件时，首先调用 SerDe（默认 LazySimpleSerDe）的 Serializer 将对象转换成字节序列，然后调用 OutputFormat 将数据写入 HDFS 文件中。

![](https://raw.githubusercontent.com/MXJULY/image/main/img/202310191430557.png)

在 Hive 的 HQL 语句中，select 时将会用到反序列化操作， insert 时会用到序列化操作。

---
