# DataStream API

Flink 提供的四种 API：

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410050215317.png)

在 Flink 中，任何数据的产生和传输过程都是数据流，而数据流就是 DataStream。

## Source

### 集合数据源

```java
public class SourceTest {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataStreamSource<Event> stream = env.fromElements(
                new Event("Alice", "www.baidu.com", 123456789L),
                new Event("Bob", "www.google.com", 123456789L),
                new Event("Alice", "www.sina.com", 123456789L)
        );
//        ArrayList<Event> events = new ArrayList<>();
//        events.add(new Event("Alice", "www.baidu.com", 123456789L));
//        events.add(new Event("Bob", "www.google.com", 123456789L));
//        events.add(new Event("Alice", "www.sina.com", 123456789L));
//        DataStreamSource<Event> stream2 = env.fromCollection(events);
        stream.print();
        env.execute();
    }
}
```

---

### Socket 数据源

```java
public class SocketTest {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataStreamSource<String> stream = env.socketTextStream("localhost", 9999);
        stream.print();
        env.execute();
    }
}
```

---

### Kafka 数据源

创建 FlinkKafkaConsumer 时需要传入三个参数：

- 第一个参数 topic，定义了从哪些主题中读取数据。可以是一个 topic，也可以是 topic 列表，还可以是匹配所有想要读取的 topic 的正则表达式。当从多个 topic 中读取数据时，Kafka 连接器将会处理所有 topic 的分区，将这些分区的数据放到一条流中去。
- 第二个参数是一个 DeserializationSchema 或者 KeyedDeserializationSchema。Kafka 消息被存储为原始的字节数据，所以需要反序列化成 Java 或者 Scala 对象。下面代码中使用的 SimpleStringSchema，是一个内置的 DeserializationSchema，它只是将字节数组简单地反序列化成字符串。
- 第三个参数是一个 Properties 对象，设置了 Kafka 客户端的一些属性。

```java
public class KafkaTest {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "consumer-group");
        properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty("auto.offset.reset", "latest");

        DataStreamSource<String> stream = env.addSource(new FlinkKafkaConsumer<String>("clicks", new SimpleStringSchema(), properties));
        stream.print("Kafka");

        env.execute();
    }
}
```

---

### 自定义数据源

创建一个自定义的数据源，需要实现 SourceFunction 接口并且重写两个关键方法：`run()` 和 `cancel()`。

- `run()`方法：使用运行时上下文对象（SourceContext）向下游发送数据
- `cancel()`方法：通过标识位控制退出循环，来达到中断数据源的效果

如果需要自定义并行的数据源（并行度>1），需要实现 ParallelSourceFunction 接口，并重写 `run()` 和 `cancel()`。

```java
public class CustomSource implements SourceFunction<Event> {
    private Boolean running = true;

    @Override
    public void run(SourceContext<Event> sourceContext) throws Exception {
        while (running) {
            sourceContext.collect(new Event("user", "url", System.currentTimeMillis()));
            Thread.sleep(1000);
        }
    }

    @Override
    public void cancel() {
        running = false;
    }
}
```

---

## Transformation

### Map

将 DataStream 中的每一个元素转换为另外一个元素。

```java
env.fromElements(1, 2, 3, 4)
    .map(x -> x * 2)
    .print();
```

### Filter

计算每个数据元的布尔函数，并保存函数返回 true 的数据元。

```java
env.fromElements(1, 2, 3, 4)
    .filter(x -> x % 2 == 0)
    .print();
```

### FlatMap

FlatMap 操作用于数据展开映射，是 Filter + Map 的组合以及扩展。FlatMap 可以实现加工输入数据并改变数据的数据类型，然后不输出数据或输出多条数据。

```java
env.fromElements("hello world", "world", "hello")
    .flatMap((String line, Collector<String> out) -> {
        for (String word : line.split(" ")) {
            if (word.equals("world")) {
                out.collect(word);
            }
        }
    })
    .returns(Types.STRING)
    .print();
```

### KeyBy

KeyBy 操作可以将一条输入 DataStream 按照 key 转换为分组的数据流 **KeyedStream**。DataStream API 提供了 `KeyedStream<T, K> keyBy(KeySelector<T, K> key)` 方法来完成数据的分组操作，其中 T 为输入的数据类型，K 为数据分组键的类型。

```java
stream.keyBy(x -> x.getSomeKey());
stream.keyBy(x -> x.f0);
```

### Max/Min/Sum

对分组的数据进行聚合。

```java
stream.keyBy(x -> x.f0).sum("f0");
```

### Reduce

对于 min/max/sum 无法处理的聚合计算逻辑，可以通过 reduce 操作来自定义数据聚合计算的逻辑。

使用时需要实现 `reduce(T acc, T in)` 方法，acc 表示该分组的历史聚合结果，in 表示该分组中新的输入数据。

```java
public interface ReduceFunction<T> extends Function, Serializable {
    T reduce(T acc, T in) throws Exception;
}
```

---

## Sink

### 控制台

```java
stream.print();
```

### Kafka

### 自定义 Sink

---

## 算子间数据传输的 8 种策略

### Forward

Forward 指上下游算子在传输时一对一的模式，是 Flink 的默认传输策略，前提是上下游算子并行度相同。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410050412007.png)

---

### Rebalance

在 Rebalance 传输策略下，上游算子会按照轮询模式传输数据给下游算子。轮询算法是 Round-Robin 负载均衡算法。RebalancePartitioner 会先随机选一个下游分区，之后轮询遍历下游所有分区进行数据传输。

在用户没有指定其他数据传输策略（如 Shuffle、Rescale 等）并且不满足 Forward 传输策略的条件下，Flink 默认使用 Rebalance 传输策略。

推荐应用场景：当一个算子的不同 SubTask 之间要处理的数据量差异很大，导致出现数据倾斜时。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410050419542.png)

---

### Shuffle

ShufflePartitioner 会随机选取下游分区进行数据传输。由于 Random 生成的随机数符合均匀分布，因此能够大致保证下发的平均效果，类似于 RebalancePartitioner。

---

### KeyGroup

KeyGroup 也称作哈希传输策略，上游算子的每一个 SubTask 中的每一条数据，会通过 `DataStream.keyBy()` 方法的入参 KeySelector 来获取 key，然后根据这个 key 经过哈希计算的结果确定发往下游算子的哪个 SubTask 中。

---

### Rescale

和 Rebalance 不同的地方在于，Rescale 并不是完全将数据轮询下发到下游算子的所有 SubTask 中，而是轮询下发到下游算子的一部分 SubTask 中。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410050428701.png)

---

### Broadcast

Broadcast 会将上游每一条数据都下发给下游算子的所有 SubTask。

---

### Global

在 Global 的传输策略下，上游算子会将所有数据下发到下游算子下标为 0 的 SubTask 中，即使下游算子的并行度大于 1。

---

### Custom Partition

自定义传输策略。使用 `DataStream.partitionCustom(Partitioner p, KeySelector k)` 方法来自定义数据传输策略，SubTask 在执行自定义传输策略时分为 2 个步骤：

1. 使用 KeySelector 获取当前数据的 key
2. 执行 Partitioner
