# Flink 基础

## Flink 简介

Apache Flink 是一个分布式流处理引擎，用于对无界和有界数据流进行有状态计算。

**特点：**

^^真正的流处理引擎^^

: 在 Flink 的流处理模式下，Flink 处理数据的粒度是事件粒度或者说数据粒度，这个特性反映到业务中就是数据处理延迟极低，一般是毫秒级别。

^^强大性能的分布式计算引擎^^

: Flink 是分布式的计算引擎，处理数据的吞吐能力能够轻松达到百万、千万级别 QPS。

^^时间语义丰富^^

: Flink 预置了多重时间语义的 API，包括事件事件、处理时间和摄入时间。

^^高可用的有状态计算引擎^^

: Flink 不但提供了丰富的状态类型和状态操作 API，而且提供了 Checkpoint、Savepoint 这样的快照机制来保障精确一次的数据处理。

^^流批一体的计算引擎^^

: Flink SQL 可以通过同一条 SQL 语句就同时完成流处理和批处理，降低开发、维护的难度。

---

## Flink 的应用场景

Flink 官方列举了 3 种 Flink 的应用场景：

- 数据同步型应用
- 数据分析型应用
- 事件驱动型应用

^^数据同步型应用^^

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410041458649.png)

^^数据分析型应用^^

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410041509610.png)

^^事件驱动型应用^^

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410041520579.png)

---

## Lambda 架构

Lambda 架构在传统周期性批处理架构的基础上添加了一个由低延迟流处理引擎所驱动的「提速层」（speed layer）。在该架构中，**到来的数据会同时发往流处理引擎和写入批量存储**。流处理引擎会近乎实时地计算出近似结果，并将其写入「提速表」中。批处理引擎周期性地处理批量存储的数据，将精确结果写入批处理表，随后将「提速表」中对应的非精确结果删除。为了获取最终结果，应用需要将「提速表」中的近似结果和批处理表中的精确结果合并。

**缺点：**

1. 该架构需要在拥有不同 API 的两套独立处理系统之上实现两套语义相同的应用逻辑

2. 流处理引擎计算的结果只是近似的

3. Lambda 架构较难配置和维护

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241522060.png)

## Flink 相关概念

### 状态

在抽象层次上，我们可以将状态视为 Flink 中算子的记忆，它记住有关过去输入的信息，并可用于影响未来输入的处理。状态是计算过程中生成的数据信息，在 Apache Flink 的容错、故障恢复和检查点中起着非常重要的作用。

### 算子

算子是数据流程序的基本功能单元，他们从输入获取数据，对其进行计算，然后产生数据并发往输出以供后续处理。**没有输入端的算子称为数据源，没有输出端的算子称为数据汇。**

![Logical Dataflow(JobGraph)](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241524222.png)

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241525096.png)

---

### 延迟和吞吐

由于流式应用会持续执行且输入可能是无限的，因此流式应用需要尽可能快地计算结果，同时还要应对很高的事件接入速率。**我们用延迟和吞吐表示这两方面的性能需求。**

- 延迟表示处理一个事件所需要的时间

- 吞吐表示系统每单位时间可以处理多少事件

- 一旦事件到达速率过高，系统就会被迫开始缓冲事件。如果系统持续以力不能及的高速率接收数据，那么缓冲区可能会用尽，继而可能导致数据丢失。**这种情形通常被称为「背压」（backpressure）。**

通过并行处理多条数据流，可以在处理更多事件的同时降低延迟。

---

### 数据流图

所有的 Flink 程序都可以归纳为由三部分构成：Source、Transformation 和 Sink。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241823628.png)

数据流图中每一条数据流（dataflow）以一个或多个 source 算子开始，以一个或多个 sink 算子结束。

---

### 并行度

在 Flink 执行过程中，每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中完全独立地执行。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241827373.png)

**一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）**。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区（stream partition）来分配并行任务。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。

并行度设置的方法，它们的优先级如下：

1. 对于一个算子，首先看在代码中是否单独指定了它的并行度，这个特定的设置优先级最高，会覆盖后面所有的设置。
2. 如果没有单独设置，那么采用当前代码中执行环境全局设置的并行度。
3. 如果代码中完全没有设置，那么采用提交时-p 参数指定的并行度。
4. 如果提交时也未指定-p 参数，那么采用集群配置文件中的默认并行度。

---

### 算子链

在 Flink 中，并行度相同的一对一（one to one）算子操作，可以直接链接在一起形成一个“大”的任务（task）。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241831916.png)

Flink 为什么要有算子链这样一个设计呢？这是因为将算子链接成 task 是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。

---

### 任务和任务槽

每个 TaskManager 是一个 JVM 的进程, 可以在不同的线程中执行一个或多个子任务。为了控制一个 worker 能接收多少个 task，worker 通过 task slot 来进行控制（一个 worker 至少有一个 task slot）。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409242027459.png)

**每个任务槽（task slot）其实表示了 TaskManager 拥有计算资源的一个固定大小的子集。这些资源就是用来独立执行一个子任务的。**

> 我们之前的 WordCount 程序设置并行度为 2 提交，一共有 5 个并行子任务，可集群即使只有 2 个 task slot 也是可以成功提交并运行的。这又是为什么呢？

这是因为默认情况下，**Flink 是允许子任务共享 slot 的**。只要属于同一个作业，那么对于不同任务节点的并行子任务，就可以放到同一个 slot 上执行。

允许 slot 共享有两个主要好处：

- 只需计算 Job 中最高并行度（parallelism）的 task slot，只要这个满足，其他的 job 也都能满足。
- 资源分配更加公平，如果有比较空闲的 slot 可以将更多的任务分配给它。图中若没有任务槽共享，负载不高的 Source/Map 等 subtask 将会占据许多资源，而负载较高的窗口 subtask 则会缺乏资源。
- 有了任务槽共享，可以将基本并行度（base parallelism）从 2 提升到 6。提高了分槽资源的利用率。同时它还可以保障 TaskManager 给 subtask 的分配的 slot 方案更加公平。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409242041693.png)

---

## Flink 组件

Flink 中的几个关键组件：

- Client
- JobManager
- TaskManager

我们的代码，实际上是由客户端获取并做转换，之后提交给 JobManger 的。JobManager 负责对作业进行中央调度管理，而它获取到要执行的作业后，会进一步处理转换，然后分发任务给众多的 TaskManager，负责具体的任务执行。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241647511.png)

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410041638535.png)

---

## Flink 的软件架构

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241520141.png)

Flink 分别提供了面向流式处理的接口（DataStream API）和面向批处理的接口（DataSet API）。因此，Flink 既可以完成流处理，也可以完成批处理。Flink 支持的拓展库涉及机器学习（FlinkML）、复杂事件处理（CEP）、以及图计算（Gelly），还有分别针对流处理和批处理的 Table API。

---

## Flink 运行时架构

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410030159309.png)

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241740652.png)

### Client

核心作用：让用户通过 Client 提交 Flink 作业去本地环境或集群环境运行。

Client 会将代码转换成“数据流图”（Dataflow Graph），并最终生成作业图（JobGraph），一并发送给 JobManager。

---

### JobManager

JobManger 包含 3 个不同的组件：

- Dispatcher
- JobMaster
- ResourceManager

通常情况下，一个 JobManager 中包含一个 Dispatcher、一个 ResourceManager 和多个 JobMaster。

^^Dispatcher^^

: Dispatcher 负责接收 Client 提交的 Flink 作业（包含 JAR 包、JobGraph 等），并为每个提交的 Flink 作业启动一个 JobMaster。在不同的部署模式和资源提供框架下提交运行 Flink 作业时，Dispatcher 组件不是必需的。

^^JobMaster^^

: JobMaster 在 Flink 作业提交阶段、部署阶段以及运行阶段都会参与。
: 在提交阶段，JobMaster 接收待执行的 Flink 作业的 JobGraph、JAR 包，然后把 JobGraph 转换成一个物理层面的 ExecutionGraph。ExecutionGraph 包含该 Flink 作业所有需要执行的 SubTask 的信息。
: 在部署阶段，JobMaster 向 ResourceManager 请求执行 Flink 作业所需要的 Task Slot 资源。当获取足够的 Task Slot 资源后，JobMaster 会将 ExecutionGraph 中的 SubTask 分发到 TaskManager 上的 Task Slot 中运行。
: 在运行阶段，JobMaster 负责所有需要协调的工作，比如协调 Flink 作业中的所有 SubTask 去执行 Checkpoint，监控每个 SubTask 的心跳，获取 SubTask 的监控指标等信息，并且会对作业执行异常、失败做出响应，负责作业的故障恢复、异常容错等工作。

^^ResourceManager^^

: ResourceManager 负责 Flink 集群中的 Task Slot 申请、管理、分配、回收，Task Slot 是 Flink 中资源调度和作业运行的最小粒度，每一个 Task Slot 都包含了一定的物理层面的 CPU 和内存资源。Task Slot 是由 TaskManager 提供并注册到 ResourceManager 上的，Flink 作业中的每一个子任务都会运行在一个 Task Slot 中。

---

### TaskManager

TaskManager 是 Flink 中执行数据处理作业的组件（Worker），每一个 TaskManager 中包含一定数量的 Task Slot。当 TaskManager 启动之后会将 Task Slot 资源注册到 ResourceManager 中，随后 TaskManager 会从 JobMaster 中接收需要部署的 SubTask，然后在 Task Slot 中启动 SubTask，随后 Flink 作业中的每一个 SubTask 开始运行，即接收数据、处理数据、产出数据。

大多数情况下，一个 Flink 作业会有多个 SubTask，这些 SubTask 会部署到多个 TaskManager 中运行，每一个 TaskManager 都会包含这个 Flink 作业的一部分 SubTask。

---

## Flink 作业提交流程

### 高层级抽象视角

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241753536.png)

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410041702439.png)

1. **提交程序代码**：编写 Flink 作业的代码后，用户将 Flink 作业提交到 Client 运行
2. **提交程序**：Client 会将代码中用户自定义的数据处理逻辑转换为 JobGraph（作业图），JobGraph 是 Flink 集群可以理解的逻辑数据流图。接下来，Client 会将程序代码、JobGraph 等信息提交到 Flink 集群的 Dispatcher（作业分发器）中
3. **分发程序**：Dispatcher 会启动一个 JobMaster 来解析 JobGraph
4. **解析程序**：JobMaster 将 JobGraph 解析为 ExecutionGraph，ExecutionGraph 是物理层面具有并行度的执行图。有了 ExecutionGraph，JobMaster 就知道执行这个 Flink 程序需要消耗多少资源了
5. **申请作业资源**：JobMaster 随后会向 ResourceManager 去申请执行这个 Flink 作业所需要的 TaskManager 资源， ResourceManager 随即向资源提供框架（如 YARN）去申请对应的资源节点
6. **启动 TaskManager**：ResourceManager 在申请到资源之后，会在这些资源节点上启动 TaskManager
7. **注册 Task Slot**：TaskManager 启动后会向 ResourceManager 注册自己可用的 Task Slot。Task Slot 是 Flink 中资源分配的最小粒度，Flink 作业的每个 SubTask 最终会运行在一个个 Task Slot 中
8. **通知提供 Task Slot**：ResourceManager 通知 TaskManager 来为当前的 Flink 作业提供可用的 Task Slot
9. **提供 Task Slot**：TaskManager 会将可以用于运行当前这个 Flink 作业的 Task Slot 告诉 JobMaster
10. **提交执行作业**：JobMaster 将 Flink 作业分发到 TaskManager 上的 Task Slot 执行
11. **作业运行**：Flink 的每个 SubTask 都在 TaskManager 上初始化并运行，进行数据处理工作

### StandAlone 模式

在独立模式（Standalone）下，只有会话模式和应用模式两种部署方式。两者整体来看流程是非常相似的：TaskManager 都需要手动启动，所以当 ResourceManager 收到 JobMaster 的请求时，会直接要求 TaskManager 提供资源。而 JobMaster 的启动时间点，会话模式是预先启动，应用模式则是在作业提交时启动。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241757459.png)

我们发现除去第 4 步不会启动 TaskManager，而且直接向已有的 TaskManager 要求资源，其他步骤与上一节所讲抽象流程完全一致。

### Yarn 集群模式

---

## Flink 部署模式

Flink 为各种场景提供了不同的部署模式，主要有以下三种：

- 会话模式（Session Mode）
- 单作业模式（Per-Job Mode）`@deprecated`
- 应用模式（Application Mode）

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202410030210636.png)

它们的区别主要在于：

- 集群的生命周期以及资源的分配方式
- 应用的 `main()` 到底在哪里执行：Client 还是 JobManager

### 会话模式

会话模式其实最符合常规思维。我们需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。

![](https://raw.githubusercontent.com/LIMUXUALE0927/image/main/img/202409241653351.png)

集群的生命周期是超越于作业之上的，作业结束了就释放资源，集群依然正常运行。当然缺点也是显而易见的：因为资源是共享的，所以资源不够了，提交新的作业就会失败。另外，同一个 TaskManager 上可能运行了很多作业，如果其中一个发生故障导致 TaskManager 宕机，那么所有作业都会受到影响。

**会话模式比较适合于单个规模小、执行时间短的大量作业**。

---

### 单作业模式

会话模式因为资源共享会导致很多问题，所以为了更好地隔离资源，我们可以考虑为每个提交的作业启动一个集群，这就是所谓的单作业（Per-Job）模式。

单作业模式也很好理解，就是严格的一对一，集群只为这个作业而生。同样由客户端运行应用程序，然后启动集群，作业被提交给 JobManager，进而分发给 TaskManager 执行。作业完成后，集群就会关闭，所有资源也会释放。这样一来，每个作业都有它自己的 JobManager 管理，占用独享的资源，即使发生故障，它的 TaskManager 宕机也不会影响其他作业。

这些特性使得单作业模式在生产环境运行更加稳定，所以是实际应用的首选模式。需要注意的是，Flink 本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如 YARN、Kubernetes。

---

### 应用模式

直接把应用提交到 JobManger 上运行。而这也就代表着，我们需要为每一个提交的应用单独启动一个 JobManager，也就是创建一个集群，但这次应用程序的 main() 方法由 JobManager 执行。为每个应用程序创建一个集群可以被视为创建一个仅在特定应用程序的作业之间共享的会话集群，并在应用程序完成时关闭。通过这种架构，应用程序模式提供与按作业模式相同的资源隔离和负载均衡保证，但粒度是整个应用程序。

应用程序模式基于一个假设，即用户的 jar 文件已经在所有需要访问它的 Flink 组件（JobManager、TaskManager）的类路径（usrlib 文件夹）中可用。换句话说，您的应用程序与 Flink 发行版捆绑在一起。这使得应用程序模式能够加快部署/恢复过程，因为它不需要像其他部署模式那样通过 RPC 将用户的 jar 文件分发到 Flink 组件。

---
